{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://packagecloud.io/JonasGroeger/soundnode/pypi/simple\n",
      "Collecting opencv-python\n",
      "  Downloading opencv_python-4.5.1.48-cp37-cp37m-manylinux2014_x86_64.whl (50.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 50.4 MB 50 kB/s s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.14.5 in /home/kivicode/anaconda3/lib/python3.7/site-packages (from opencv-python) (1.17.2)\n",
      "Installing collected packages: opencv-python\n",
      "Successfully installed opencv-python-4.5.1.48\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# example of horizontal shift image augmentation\n",
    "\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import to_categorical\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Lambda, Dropout, Reshape\n",
    "from keras.optimizers import SGD\n",
    "import tensorflow as tf\n",
    "import random as rng\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/kivicode/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 24, 24, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 12, 12, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 12, 12, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 10, 10, 16)        4624      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 5, 5, 16)          0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 5, 5, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 3, 3, 8)           1160      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 1, 1, 8)           0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 1, 1, 8)           0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 300)               2700      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 150)               45150     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 150)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 5)                 755       \n",
      "=================================================================\n",
      "Total params: 63,957\n",
      "Trainable params: 63,957\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Building the convolutional neural network\n",
    "\n",
    "classifier = Sequential()\n",
    "\n",
    "# Adding the first 2 convolution layer\n",
    "\n",
    "classifier.add(Conv2D(32, (3, 3), input_shape=(28, 28, 1), activation='relu'))\n",
    "\n",
    "classifier.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "classifier.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "classifier.add(Dropout(.7))\n",
    "\n",
    "classifier.add(Conv2D(16, (3, 3), activation='relu'))\n",
    "classifier.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "classifier.add(Dropout(.7))\n",
    "\n",
    "classifier.add(Conv2D(8, (3, 3), activation='relu'))\n",
    "classifier.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "classifier.add(Dropout(.7))\n",
    "\n",
    "# Adding the flattening layer\n",
    "\n",
    "classifier.add(Flatten())\n",
    "\n",
    "# Adding the ANN\n",
    "\n",
    "classifier.add(Dense(units=300, activation='relu'))\n",
    "classifier.add(Dropout(rate=0.5))\n",
    "\n",
    "classifier.add(Dense(units=150, activation='relu'))\n",
    "classifier.add(Dropout(rate=0.5))\n",
    "classifier.add(Dense(units=5, activation='softmax'))\n",
    "\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='accuracy', factor=0.25, patience=2, min_lr=0.0001)\n",
    "classifier.compile(optimizer=Adam(lr=0.002), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "classifier.summary()\n",
    "# hist = classifier.fit_generator(train_it, \n",
    "#                          steps_per_epoch=1000,\n",
    "#                          epochs=20,\n",
    "#                          validation_data=(x_test,y_test),callbacks=[reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create image data augmentation generator\n",
    "datagen = ImageDataGenerator(brightness_range=[0.5,1.0], \n",
    "                             zoom_range=[.85,.95],\n",
    "                             width_shift_range=0.15,\n",
    "                             height_shift_range=0.15,\n",
    "                             fill_mode='constant',\n",
    "                             cval=0,\n",
    "                             validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 5) (1000, 28, 28, 1)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 9 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "xx = []\n",
    "yy = []\n",
    "\n",
    "x_train, y_train = [], []\n",
    "for j in range(1000):\n",
    "    for i in range(5):\n",
    "        pad = np.zeros((28, 28))\n",
    "        fnt = rng.randint(0,7)\n",
    "        scale = 1\n",
    "        if fnt in [1, 5]:\n",
    "            pass\n",
    "        else:\n",
    "            scale = 0.7\n",
    "        pad = cv2.putText(pad, str(i), (7-rng.randint(-3,3),20-rng.randint(-3,3)), fnt,  \n",
    "                        scale, 255, 1, cv2.LINE_AA)\n",
    "        y_train.append(i)\n",
    "        x_train.append(pad)\n",
    "x_train = np.array(x_train).reshape(-1, 28, 28, 1)\n",
    "y_train = to_categorical(np.array(y_train))\n",
    "gen = datagen.flow(x_train, y_train, batch_size=1)\n",
    "\n",
    "for i in range(1000):\n",
    "    cur = gen.next()\n",
    "    _x = cur[0].reshape(-1, 28, 28)\n",
    "    _y = cur[1]\n",
    "    xx.append(np.array([cv2.rotate(cv2.threshold(im.astype('uint8'), 10, 255, cv2.THRESH_OTSU)[1], \n",
    "                        rng.choice([0, cv2.ROTATE_180, cv2.ROTATE_90_CLOCKWISE, cv2.ROTATE_90_COUNTERCLOCKWISE])) for im in _x ]))\n",
    "    yy.append(_y)\n",
    "\n",
    "x = np.asarray(xx)\n",
    "x = x.reshape(-1, 28, 28, 1) / 255.\n",
    "y = np.array(yy).reshape(-1, 5)\n",
    "\n",
    "print(y.shape, x.shape)\n",
    "\n",
    "# generate samples and plot\n",
    "for i in range(9):\n",
    "    # define subplot\n",
    "    plt.subplot(330 + 1 + i)\n",
    "    \n",
    "    k = rng.randint(1, 1000)\n",
    "  \n",
    "    plt.imshow(x[k].reshape(28,28))\n",
    "    plt.title(y[k].argmax())\n",
    "# show the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 800 samples, validate on 200 samples\n",
      "Epoch 1/600\n",
      "800/800 [==============================] - 0s 270us/step - loss: 0.5354 - accuracy: 0.8188 - val_loss: 2.1856 - val_accuracy: 0.1800\n",
      "Epoch 2/600\n",
      "800/800 [==============================] - 0s 284us/step - loss: 0.5242 - accuracy: 0.8225 - val_loss: 2.2025 - val_accuracy: 0.1800\n",
      "Epoch 3/600\n",
      "800/800 [==============================] - 0s 291us/step - loss: 0.5363 - accuracy: 0.8400 - val_loss: 2.3026 - val_accuracy: 0.1750\n",
      "Epoch 4/600\n",
      "800/800 [==============================] - 0s 298us/step - loss: 0.5112 - accuracy: 0.8188 - val_loss: 2.1644 - val_accuracy: 0.1800\n",
      "Epoch 5/600\n",
      "800/800 [==============================] - 0s 288us/step - loss: 0.5688 - accuracy: 0.8263 - val_loss: 2.2902 - val_accuracy: 0.1800\n",
      "Epoch 6/600\n",
      "800/800 [==============================] - 0s 293us/step - loss: 0.5694 - accuracy: 0.8163 - val_loss: 2.1498 - val_accuracy: 0.1800\n",
      "Epoch 7/600\n",
      "800/800 [==============================] - 0s 259us/step - loss: 0.5595 - accuracy: 0.8225 - val_loss: 2.3367 - val_accuracy: 0.1700\n",
      "Epoch 8/600\n",
      "800/800 [==============================] - 0s 287us/step - loss: 0.4796 - accuracy: 0.8438 - val_loss: 2.1028 - val_accuracy: 0.1850\n",
      "Epoch 9/600\n",
      "800/800 [==============================] - 0s 288us/step - loss: 0.5535 - accuracy: 0.8225 - val_loss: 2.1264 - val_accuracy: 0.1900\n",
      "Epoch 10/600\n",
      "800/800 [==============================] - 0s 274us/step - loss: 0.5546 - accuracy: 0.8125 - val_loss: 2.0798 - val_accuracy: 0.1900\n",
      "Epoch 11/600\n",
      "800/800 [==============================] - 0s 277us/step - loss: 0.4968 - accuracy: 0.8475 - val_loss: 2.1331 - val_accuracy: 0.1800\n",
      "Epoch 12/600\n",
      "800/800 [==============================] - 0s 287us/step - loss: 0.5555 - accuracy: 0.8200 - val_loss: 2.2681 - val_accuracy: 0.1700\n",
      "Epoch 13/600\n",
      "800/800 [==============================] - 0s 288us/step - loss: 0.5306 - accuracy: 0.8250 - val_loss: 1.8901 - val_accuracy: 0.2500\n",
      "Epoch 14/600\n",
      "800/800 [==============================] - 0s 285us/step - loss: 0.5264 - accuracy: 0.8388 - val_loss: 2.1468 - val_accuracy: 0.1850\n",
      "Epoch 15/600\n",
      "800/800 [==============================] - 0s 300us/step - loss: 0.5620 - accuracy: 0.8150 - val_loss: 2.1174 - val_accuracy: 0.1900\n",
      "Epoch 16/600\n",
      "800/800 [==============================] - 0s 275us/step - loss: 0.4708 - accuracy: 0.8612 - val_loss: 2.0365 - val_accuracy: 0.2100\n",
      "Epoch 17/600\n",
      "800/800 [==============================] - 0s 261us/step - loss: 0.5336 - accuracy: 0.8313 - val_loss: 2.0148 - val_accuracy: 0.2100\n",
      "Epoch 18/600\n",
      "800/800 [==============================] - 0s 286us/step - loss: 0.4862 - accuracy: 0.8425 - val_loss: 2.0157 - val_accuracy: 0.2250\n",
      "Epoch 19/600\n",
      "800/800 [==============================] - 0s 305us/step - loss: 0.5218 - accuracy: 0.8325 - val_loss: 2.0063 - val_accuracy: 0.2050\n",
      "Epoch 20/600\n",
      "800/800 [==============================] - 0s 266us/step - loss: 0.5973 - accuracy: 0.8125 - val_loss: 2.1645 - val_accuracy: 0.1750\n",
      "Epoch 21/600\n",
      "800/800 [==============================] - 0s 275us/step - loss: 0.4945 - accuracy: 0.8438 - val_loss: 1.9411 - val_accuracy: 0.2100\n",
      "Epoch 22/600\n",
      "800/800 [==============================] - 0s 293us/step - loss: 0.5366 - accuracy: 0.8200 - val_loss: 2.1651 - val_accuracy: 0.1750\n",
      "Epoch 23/600\n",
      "800/800 [==============================] - 0s 284us/step - loss: 0.5050 - accuracy: 0.8375 - val_loss: 2.0747 - val_accuracy: 0.1800\n",
      "Epoch 24/600\n",
      "800/800 [==============================] - 0s 295us/step - loss: 0.5086 - accuracy: 0.8275 - val_loss: 2.1591 - val_accuracy: 0.1900\n",
      "Epoch 25/600\n",
      "800/800 [==============================] - 0s 291us/step - loss: 0.5885 - accuracy: 0.8062 - val_loss: 2.1510 - val_accuracy: 0.1850\n",
      "Epoch 26/600\n",
      "800/800 [==============================] - 0s 296us/step - loss: 0.5314 - accuracy: 0.8263 - val_loss: 2.0631 - val_accuracy: 0.1950\n",
      "Epoch 27/600\n",
      "800/800 [==============================] - 0s 284us/step - loss: 0.5679 - accuracy: 0.8062 - val_loss: 2.2793 - val_accuracy: 0.1750\n",
      "Epoch 28/600\n",
      "800/800 [==============================] - 0s 281us/step - loss: 0.5519 - accuracy: 0.8175 - val_loss: 2.0498 - val_accuracy: 0.2100\n",
      "Epoch 29/600\n",
      "800/800 [==============================] - 0s 308us/step - loss: 0.5675 - accuracy: 0.8012 - val_loss: 1.8962 - val_accuracy: 0.2700\n",
      "Epoch 30/600\n",
      "800/800 [==============================] - 0s 274us/step - loss: 0.5251 - accuracy: 0.8188 - val_loss: 2.0121 - val_accuracy: 0.1850\n",
      "Epoch 31/600\n",
      "800/800 [==============================] - 0s 271us/step - loss: 0.5299 - accuracy: 0.8363 - val_loss: 2.0887 - val_accuracy: 0.2150\n",
      "Epoch 32/600\n",
      "800/800 [==============================] - 0s 304us/step - loss: 0.5034 - accuracy: 0.8400 - val_loss: 2.2436 - val_accuracy: 0.1800\n",
      "Epoch 33/600\n",
      "800/800 [==============================] - 0s 292us/step - loss: 0.5145 - accuracy: 0.8388 - val_loss: 2.1217 - val_accuracy: 0.1950\n",
      "Epoch 34/600\n",
      "800/800 [==============================] - 0s 297us/step - loss: 0.4963 - accuracy: 0.8388 - val_loss: 2.0333 - val_accuracy: 0.1900\n",
      "Epoch 35/600\n",
      "800/800 [==============================] - 0s 299us/step - loss: 0.5203 - accuracy: 0.8350 - val_loss: 2.0382 - val_accuracy: 0.2000\n",
      "Epoch 36/600\n",
      "800/800 [==============================] - 0s 279us/step - loss: 0.5223 - accuracy: 0.8225 - val_loss: 2.1159 - val_accuracy: 0.1850\n",
      "Epoch 37/600\n",
      "800/800 [==============================] - 0s 294us/step - loss: 0.5279 - accuracy: 0.8238 - val_loss: 1.9467 - val_accuracy: 0.2050\n",
      "Epoch 38/600\n",
      "800/800 [==============================] - 0s 304us/step - loss: 0.5403 - accuracy: 0.8238 - val_loss: 2.0070 - val_accuracy: 0.2000\n",
      "Epoch 39/600\n",
      "800/800 [==============================] - 0s 279us/step - loss: 0.5887 - accuracy: 0.8100 - val_loss: 2.1067 - val_accuracy: 0.1950\n",
      "Epoch 40/600\n",
      "800/800 [==============================] - 0s 268us/step - loss: 0.5647 - accuracy: 0.8188 - val_loss: 2.0548 - val_accuracy: 0.1950\n",
      "Epoch 41/600\n",
      "800/800 [==============================] - 0s 286us/step - loss: 0.5092 - accuracy: 0.8288 - val_loss: 1.9487 - val_accuracy: 0.2200\n",
      "Epoch 42/600\n",
      "800/800 [==============================] - 0s 276us/step - loss: 0.4829 - accuracy: 0.8425 - val_loss: 1.9081 - val_accuracy: 0.2250\n",
      "Epoch 43/600\n",
      "800/800 [==============================] - 0s 301us/step - loss: 0.5563 - accuracy: 0.8150 - val_loss: 2.0538 - val_accuracy: 0.1950\n",
      "Epoch 44/600\n",
      "800/800 [==============================] - 0s 277us/step - loss: 0.5503 - accuracy: 0.8175 - val_loss: 2.0620 - val_accuracy: 0.1950\n",
      "Epoch 45/600\n",
      "800/800 [==============================] - 0s 276us/step - loss: 0.5503 - accuracy: 0.8275 - val_loss: 1.9390 - val_accuracy: 0.2400\n",
      "Epoch 46/600\n",
      "800/800 [==============================] - 0s 304us/step - loss: 0.5443 - accuracy: 0.8325 - val_loss: 1.9139 - val_accuracy: 0.2050\n",
      "Epoch 47/600\n",
      "800/800 [==============================] - 0s 280us/step - loss: 0.5203 - accuracy: 0.8462 - val_loss: 1.9506 - val_accuracy: 0.2400\n",
      "Epoch 48/600\n",
      "800/800 [==============================] - 0s 269us/step - loss: 0.4940 - accuracy: 0.8425 - val_loss: 1.9248 - val_accuracy: 0.2400\n",
      "Epoch 49/600\n",
      "800/800 [==============================] - 0s 271us/step - loss: 0.5429 - accuracy: 0.8213 - val_loss: 1.9734 - val_accuracy: 0.2350\n",
      "Epoch 50/600\n",
      "800/800 [==============================] - 0s 286us/step - loss: 0.4764 - accuracy: 0.8425 - val_loss: 2.0698 - val_accuracy: 0.1900\n",
      "Epoch 51/600\n",
      "800/800 [==============================] - 0s 277us/step - loss: 0.5273 - accuracy: 0.8250 - val_loss: 2.0721 - val_accuracy: 0.2200\n",
      "Epoch 52/600\n",
      "800/800 [==============================] - 0s 279us/step - loss: 0.5681 - accuracy: 0.8125 - val_loss: 1.9808 - val_accuracy: 0.2450\n",
      "Epoch 53/600\n",
      "800/800 [==============================] - 0s 285us/step - loss: 0.5257 - accuracy: 0.8238 - val_loss: 1.9973 - val_accuracy: 0.2350\n",
      "Epoch 54/600\n",
      "800/800 [==============================] - 0s 281us/step - loss: 0.5358 - accuracy: 0.8400 - val_loss: 1.9451 - val_accuracy: 0.2450\n",
      "Epoch 55/600\n",
      "800/800 [==============================] - 0s 267us/step - loss: 0.5192 - accuracy: 0.8388 - val_loss: 2.0253 - val_accuracy: 0.2050\n",
      "Epoch 56/600\n",
      "800/800 [==============================] - 0s 276us/step - loss: 0.5399 - accuracy: 0.8225 - val_loss: 1.9577 - val_accuracy: 0.2550\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/600\n",
      "800/800 [==============================] - 0s 289us/step - loss: 0.5177 - accuracy: 0.8338 - val_loss: 2.0070 - val_accuracy: 0.1950\n",
      "Epoch 58/600\n",
      "800/800 [==============================] - 0s 279us/step - loss: 0.5021 - accuracy: 0.8263 - val_loss: 1.9562 - val_accuracy: 0.2150\n",
      "Epoch 59/600\n",
      "800/800 [==============================] - 0s 257us/step - loss: 0.5063 - accuracy: 0.8375 - val_loss: 2.0459 - val_accuracy: 0.1950\n",
      "Epoch 60/600\n",
      "800/800 [==============================] - 0s 276us/step - loss: 0.4955 - accuracy: 0.8413 - val_loss: 1.9788 - val_accuracy: 0.2050\n",
      "Epoch 61/600\n",
      "800/800 [==============================] - 0s 258us/step - loss: 0.5172 - accuracy: 0.8263 - val_loss: 1.9200 - val_accuracy: 0.2450\n",
      "Epoch 62/600\n",
      "800/800 [==============================] - 0s 267us/step - loss: 0.5392 - accuracy: 0.8263 - val_loss: 2.0394 - val_accuracy: 0.2100\n",
      "Epoch 63/600\n",
      "800/800 [==============================] - 0s 303us/step - loss: 0.5031 - accuracy: 0.8388 - val_loss: 2.0962 - val_accuracy: 0.1900\n",
      "Epoch 64/600\n",
      "800/800 [==============================] - 0s 290us/step - loss: 0.5184 - accuracy: 0.8413 - val_loss: 1.9085 - val_accuracy: 0.2300\n",
      "Epoch 65/600\n",
      "800/800 [==============================] - 0s 270us/step - loss: 0.5535 - accuracy: 0.8288 - val_loss: 1.9310 - val_accuracy: 0.2400\n",
      "Epoch 66/600\n",
      "800/800 [==============================] - 0s 285us/step - loss: 0.5667 - accuracy: 0.8075 - val_loss: 2.2975 - val_accuracy: 0.1700\n",
      "Epoch 67/600\n",
      "800/800 [==============================] - 0s 266us/step - loss: 0.4921 - accuracy: 0.8413 - val_loss: 1.9189 - val_accuracy: 0.2250\n",
      "Epoch 68/600\n",
      "800/800 [==============================] - 0s 270us/step - loss: 0.5411 - accuracy: 0.8250 - val_loss: 1.9536 - val_accuracy: 0.2400\n",
      "Epoch 69/600\n",
      "800/800 [==============================] - 0s 285us/step - loss: 0.5512 - accuracy: 0.8175 - val_loss: 2.1281 - val_accuracy: 0.1800\n",
      "Epoch 70/600\n",
      "800/800 [==============================] - 0s 303us/step - loss: 0.5311 - accuracy: 0.8413 - val_loss: 2.1748 - val_accuracy: 0.1800\n",
      "Epoch 71/600\n",
      "800/800 [==============================] - 0s 282us/step - loss: 0.5435 - accuracy: 0.8250 - val_loss: 2.0222 - val_accuracy: 0.2250\n",
      "Epoch 72/600\n",
      "800/800 [==============================] - 0s 288us/step - loss: 0.5517 - accuracy: 0.8238 - val_loss: 2.2367 - val_accuracy: 0.1850\n",
      "Epoch 73/600\n",
      "800/800 [==============================] - 0s 275us/step - loss: 0.5326 - accuracy: 0.8325 - val_loss: 2.1243 - val_accuracy: 0.1950\n",
      "Epoch 74/600\n",
      "800/800 [==============================] - 0s 303us/step - loss: 0.5385 - accuracy: 0.8188 - val_loss: 2.0496 - val_accuracy: 0.1900\n",
      "Epoch 75/600\n",
      "800/800 [==============================] - 0s 270us/step - loss: 0.5082 - accuracy: 0.8388 - val_loss: 2.0062 - val_accuracy: 0.2050\n",
      "Epoch 76/600\n",
      "800/800 [==============================] - 0s 289us/step - loss: 0.5021 - accuracy: 0.8250 - val_loss: 2.2852 - val_accuracy: 0.1700\n",
      "Epoch 77/600\n",
      "800/800 [==============================] - 0s 306us/step - loss: 0.5644 - accuracy: 0.8150 - val_loss: 1.9780 - val_accuracy: 0.2200\n",
      "Epoch 78/600\n",
      "800/800 [==============================] - 0s 290us/step - loss: 0.4911 - accuracy: 0.8450 - val_loss: 2.1077 - val_accuracy: 0.2000\n",
      "Epoch 79/600\n",
      "800/800 [==============================] - 0s 283us/step - loss: 0.5279 - accuracy: 0.8288 - val_loss: 1.9336 - val_accuracy: 0.2200\n",
      "Epoch 80/600\n",
      "800/800 [==============================] - 0s 269us/step - loss: 0.5795 - accuracy: 0.8050 - val_loss: 2.0570 - val_accuracy: 0.2100\n",
      "Epoch 81/600\n",
      "800/800 [==============================] - 0s 299us/step - loss: 0.5271 - accuracy: 0.8200 - val_loss: 2.0565 - val_accuracy: 0.2050\n",
      "Epoch 82/600\n",
      "800/800 [==============================] - 0s 286us/step - loss: 0.5013 - accuracy: 0.8487 - val_loss: 2.1095 - val_accuracy: 0.1950\n",
      "Epoch 83/600\n",
      "800/800 [==============================] - 0s 312us/step - loss: 0.4977 - accuracy: 0.8425 - val_loss: 1.9788 - val_accuracy: 0.2550\n",
      "Epoch 84/600\n",
      "800/800 [==============================] - 0s 298us/step - loss: 0.5173 - accuracy: 0.8338 - val_loss: 2.0088 - val_accuracy: 0.2100\n",
      "Epoch 85/600\n",
      "800/800 [==============================] - 0s 291us/step - loss: 0.5098 - accuracy: 0.8400 - val_loss: 2.0311 - val_accuracy: 0.2400\n",
      "Epoch 86/600\n",
      "800/800 [==============================] - 0s 283us/step - loss: 0.5257 - accuracy: 0.8388 - val_loss: 1.9184 - val_accuracy: 0.2600\n",
      "Epoch 87/600\n",
      "800/800 [==============================] - 0s 275us/step - loss: 0.5020 - accuracy: 0.8363 - val_loss: 2.0759 - val_accuracy: 0.2050\n",
      "Epoch 88/600\n",
      "800/800 [==============================] - 0s 269us/step - loss: 0.5312 - accuracy: 0.8313 - val_loss: 2.0568 - val_accuracy: 0.2050\n",
      "Epoch 89/600\n",
      "800/800 [==============================] - 0s 274us/step - loss: 0.5378 - accuracy: 0.8163 - val_loss: 1.8633 - val_accuracy: 0.2800\n",
      "Epoch 90/600\n",
      "800/800 [==============================] - 0s 292us/step - loss: 0.5154 - accuracy: 0.8300 - val_loss: 2.0640 - val_accuracy: 0.2200\n",
      "Epoch 91/600\n",
      "800/800 [==============================] - 0s 273us/step - loss: 0.4592 - accuracy: 0.8487 - val_loss: 1.9017 - val_accuracy: 0.2650\n",
      "Epoch 92/600\n",
      "800/800 [==============================] - 0s 285us/step - loss: 0.4993 - accuracy: 0.8462 - val_loss: 1.8610 - val_accuracy: 0.2600\n",
      "Epoch 93/600\n",
      "800/800 [==============================] - 0s 276us/step - loss: 0.4842 - accuracy: 0.8350 - val_loss: 1.9446 - val_accuracy: 0.2650\n",
      "Epoch 94/600\n",
      "800/800 [==============================] - 0s 286us/step - loss: 0.5015 - accuracy: 0.8512 - val_loss: 2.0196 - val_accuracy: 0.2200\n",
      "Epoch 95/600\n",
      "800/800 [==============================] - 0s 287us/step - loss: 0.5146 - accuracy: 0.8238 - val_loss: 1.9425 - val_accuracy: 0.2350\n",
      "Epoch 96/600\n",
      "800/800 [==============================] - 0s 285us/step - loss: 0.4752 - accuracy: 0.8438 - val_loss: 2.0643 - val_accuracy: 0.2100\n",
      "Epoch 97/600\n",
      "800/800 [==============================] - 0s 268us/step - loss: 0.5020 - accuracy: 0.8413 - val_loss: 1.9476 - val_accuracy: 0.2250\n",
      "Epoch 98/600\n",
      "800/800 [==============================] - 0s 288us/step - loss: 0.5329 - accuracy: 0.8175 - val_loss: 2.0576 - val_accuracy: 0.2000\n",
      "Epoch 99/600\n",
      "800/800 [==============================] - 0s 285us/step - loss: 0.5139 - accuracy: 0.8475 - val_loss: 2.1638 - val_accuracy: 0.1950\n",
      "Epoch 100/600\n",
      "800/800 [==============================] - 0s 286us/step - loss: 0.5669 - accuracy: 0.8213 - val_loss: 1.8391 - val_accuracy: 0.2250\n",
      "Epoch 101/600\n",
      "800/800 [==============================] - 0s 286us/step - loss: 0.5231 - accuracy: 0.8388 - val_loss: 1.9898 - val_accuracy: 0.2150\n",
      "Epoch 102/600\n",
      "800/800 [==============================] - 0s 292us/step - loss: 0.5087 - accuracy: 0.8375 - val_loss: 2.0253 - val_accuracy: 0.1950\n",
      "Epoch 103/600\n",
      "800/800 [==============================] - 0s 290us/step - loss: 0.5043 - accuracy: 0.8363 - val_loss: 1.9013 - val_accuracy: 0.2150\n",
      "Epoch 104/600\n",
      "800/800 [==============================] - 0s 277us/step - loss: 0.5920 - accuracy: 0.8050 - val_loss: 2.1130 - val_accuracy: 0.1850\n",
      "Epoch 105/600\n",
      "800/800 [==============================] - 0s 269us/step - loss: 0.5011 - accuracy: 0.8512 - val_loss: 2.0402 - val_accuracy: 0.2050\n",
      "Epoch 106/600\n",
      "800/800 [==============================] - 0s 297us/step - loss: 0.5225 - accuracy: 0.8300 - val_loss: 2.0261 - val_accuracy: 0.2150\n",
      "Epoch 107/600\n",
      "800/800 [==============================] - 0s 285us/step - loss: 0.4836 - accuracy: 0.8550 - val_loss: 1.9276 - val_accuracy: 0.2150\n",
      "Epoch 108/600\n",
      "800/800 [==============================] - 0s 280us/step - loss: 0.5138 - accuracy: 0.8363 - val_loss: 1.9950 - val_accuracy: 0.2400\n",
      "Epoch 109/600\n",
      "800/800 [==============================] - 0s 277us/step - loss: 0.4663 - accuracy: 0.8600 - val_loss: 2.0057 - val_accuracy: 0.2150\n",
      "Epoch 110/600\n",
      "800/800 [==============================] - 0s 259us/step - loss: 0.4943 - accuracy: 0.8288 - val_loss: 2.0269 - val_accuracy: 0.2050\n",
      "Epoch 111/600\n",
      "800/800 [==============================] - 0s 261us/step - loss: 0.4760 - accuracy: 0.8600 - val_loss: 2.0467 - val_accuracy: 0.2300\n",
      "Epoch 112/600\n",
      "800/800 [==============================] - 0s 294us/step - loss: 0.5115 - accuracy: 0.8388 - val_loss: 2.0417 - val_accuracy: 0.2100\n",
      "Epoch 113/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800/800 [==============================] - 0s 276us/step - loss: 0.5601 - accuracy: 0.8188 - val_loss: 1.9546 - val_accuracy: 0.2250\n",
      "Epoch 114/600\n",
      "800/800 [==============================] - 0s 296us/step - loss: 0.4803 - accuracy: 0.8425 - val_loss: 1.9629 - val_accuracy: 0.2600\n",
      "Epoch 115/600\n",
      "800/800 [==============================] - 0s 269us/step - loss: 0.4625 - accuracy: 0.8512 - val_loss: 2.0361 - val_accuracy: 0.2200\n",
      "Epoch 116/600\n",
      "800/800 [==============================] - 0s 307us/step - loss: 0.4993 - accuracy: 0.8375 - val_loss: 1.9570 - val_accuracy: 0.2750\n",
      "Epoch 117/600\n",
      "800/800 [==============================] - 0s 296us/step - loss: 0.5046 - accuracy: 0.8263 - val_loss: 2.0355 - val_accuracy: 0.2450\n",
      "Epoch 118/600\n",
      "800/800 [==============================] - 0s 281us/step - loss: 0.4863 - accuracy: 0.8300 - val_loss: 2.0094 - val_accuracy: 0.2350\n",
      "Epoch 119/600\n",
      "800/800 [==============================] - 0s 285us/step - loss: 0.4832 - accuracy: 0.8475 - val_loss: 2.1543 - val_accuracy: 0.1850\n",
      "Epoch 120/600\n",
      "800/800 [==============================] - 0s 257us/step - loss: 0.5055 - accuracy: 0.8462 - val_loss: 2.1894 - val_accuracy: 0.1750\n",
      "Epoch 121/600\n",
      "800/800 [==============================] - 0s 300us/step - loss: 0.4892 - accuracy: 0.8438 - val_loss: 2.1168 - val_accuracy: 0.2050\n",
      "Epoch 122/600\n",
      "800/800 [==============================] - 0s 277us/step - loss: 0.5128 - accuracy: 0.8438 - val_loss: 2.0733 - val_accuracy: 0.2000\n",
      "Epoch 123/600\n",
      "800/800 [==============================] - 0s 303us/step - loss: 0.5081 - accuracy: 0.8388 - val_loss: 2.1301 - val_accuracy: 0.1900\n",
      "Epoch 124/600\n",
      "800/800 [==============================] - 0s 298us/step - loss: 0.4846 - accuracy: 0.8562 - val_loss: 2.0954 - val_accuracy: 0.2100\n",
      "Epoch 125/600\n",
      "800/800 [==============================] - 0s 299us/step - loss: 0.5033 - accuracy: 0.8438 - val_loss: 1.9127 - val_accuracy: 0.2200\n",
      "Epoch 126/600\n",
      "800/800 [==============================] - 0s 288us/step - loss: 0.4677 - accuracy: 0.8612 - val_loss: 2.0881 - val_accuracy: 0.1950\n",
      "Epoch 127/600\n",
      "800/800 [==============================] - 0s 296us/step - loss: 0.4783 - accuracy: 0.8462 - val_loss: 1.9596 - val_accuracy: 0.2050\n",
      "Epoch 128/600\n",
      "800/800 [==============================] - 0s 288us/step - loss: 0.4957 - accuracy: 0.8462 - val_loss: 2.0147 - val_accuracy: 0.2450\n",
      "Epoch 129/600\n",
      "800/800 [==============================] - 0s 282us/step - loss: 0.4738 - accuracy: 0.8475 - val_loss: 2.1762 - val_accuracy: 0.1950\n",
      "Epoch 130/600\n",
      "800/800 [==============================] - 0s 280us/step - loss: 0.4602 - accuracy: 0.8562 - val_loss: 1.9743 - val_accuracy: 0.2300\n",
      "Epoch 131/600\n",
      "800/800 [==============================] - 0s 291us/step - loss: 0.5080 - accuracy: 0.8350 - val_loss: 2.0327 - val_accuracy: 0.2250\n",
      "Epoch 132/600\n",
      "800/800 [==============================] - 0s 288us/step - loss: 0.4965 - accuracy: 0.8450 - val_loss: 2.0606 - val_accuracy: 0.2200\n",
      "Epoch 133/600\n",
      "800/800 [==============================] - 0s 297us/step - loss: 0.4999 - accuracy: 0.8288 - val_loss: 1.9058 - val_accuracy: 0.2400\n",
      "Epoch 134/600\n",
      "800/800 [==============================] - 0s 284us/step - loss: 0.5479 - accuracy: 0.8163 - val_loss: 1.9563 - val_accuracy: 0.2400\n",
      "Epoch 135/600\n",
      "800/800 [==============================] - 0s 296us/step - loss: 0.4913 - accuracy: 0.8475 - val_loss: 1.8553 - val_accuracy: 0.2550\n",
      "Epoch 136/600\n",
      "800/800 [==============================] - 0s 260us/step - loss: 0.4873 - accuracy: 0.8438 - val_loss: 2.0567 - val_accuracy: 0.1900\n",
      "Epoch 137/600\n",
      "800/800 [==============================] - 0s 295us/step - loss: 0.5579 - accuracy: 0.8150 - val_loss: 1.9304 - val_accuracy: 0.2250\n",
      "Epoch 138/600\n",
      "800/800 [==============================] - 0s 284us/step - loss: 0.4777 - accuracy: 0.8525 - val_loss: 2.0737 - val_accuracy: 0.2100\n",
      "Epoch 139/600\n",
      "800/800 [==============================] - 0s 282us/step - loss: 0.4898 - accuracy: 0.8300 - val_loss: 2.0367 - val_accuracy: 0.2050\n",
      "Epoch 140/600\n",
      "800/800 [==============================] - 0s 296us/step - loss: 0.4962 - accuracy: 0.8450 - val_loss: 2.0163 - val_accuracy: 0.2200\n",
      "Epoch 141/600\n",
      "800/800 [==============================] - 0s 287us/step - loss: 0.5155 - accuracy: 0.8363 - val_loss: 1.9355 - val_accuracy: 0.2300\n",
      "Epoch 142/600\n",
      "800/800 [==============================] - 0s 296us/step - loss: 0.5163 - accuracy: 0.8363 - val_loss: 2.0659 - val_accuracy: 0.1950\n",
      "Epoch 143/600\n",
      "800/800 [==============================] - 0s 295us/step - loss: 0.5254 - accuracy: 0.8413 - val_loss: 2.1357 - val_accuracy: 0.1900\n",
      "Epoch 144/600\n",
      "800/800 [==============================] - 0s 303us/step - loss: 0.4857 - accuracy: 0.8450 - val_loss: 1.9772 - val_accuracy: 0.2200\n",
      "Epoch 145/600\n",
      "800/800 [==============================] - 0s 300us/step - loss: 0.5058 - accuracy: 0.8350 - val_loss: 1.9734 - val_accuracy: 0.2450\n",
      "Epoch 146/600\n",
      "800/800 [==============================] - 0s 281us/step - loss: 0.5287 - accuracy: 0.8413 - val_loss: 2.0050 - val_accuracy: 0.2150\n",
      "Epoch 147/600\n",
      "800/800 [==============================] - 0s 291us/step - loss: 0.5118 - accuracy: 0.8275 - val_loss: 2.0982 - val_accuracy: 0.1900\n",
      "Epoch 148/600\n",
      "800/800 [==============================] - 0s 302us/step - loss: 0.4778 - accuracy: 0.8525 - val_loss: 1.9327 - val_accuracy: 0.2300\n",
      "Epoch 149/600\n",
      "800/800 [==============================] - 0s 288us/step - loss: 0.4854 - accuracy: 0.8537 - val_loss: 2.0473 - val_accuracy: 0.2000\n",
      "Epoch 150/600\n",
      "800/800 [==============================] - 0s 282us/step - loss: 0.4781 - accuracy: 0.8450 - val_loss: 1.9989 - val_accuracy: 0.2150\n",
      "Epoch 151/600\n",
      "800/800 [==============================] - 0s 277us/step - loss: 0.5026 - accuracy: 0.8325 - val_loss: 2.1328 - val_accuracy: 0.1950\n",
      "Epoch 152/600\n",
      "800/800 [==============================] - 0s 296us/step - loss: 0.5031 - accuracy: 0.8263 - val_loss: 1.9846 - val_accuracy: 0.2300\n",
      "Epoch 153/600\n",
      "800/800 [==============================] - 0s 288us/step - loss: 0.5551 - accuracy: 0.8250 - val_loss: 1.9128 - val_accuracy: 0.2450\n",
      "Epoch 154/600\n",
      "800/800 [==============================] - 0s 287us/step - loss: 0.4977 - accuracy: 0.8350 - val_loss: 2.2115 - val_accuracy: 0.1900\n",
      "Epoch 155/600\n",
      "800/800 [==============================] - 0s 297us/step - loss: 0.4486 - accuracy: 0.8525 - val_loss: 2.0063 - val_accuracy: 0.2350\n",
      "Epoch 156/600\n",
      "800/800 [==============================] - 0s 299us/step - loss: 0.4954 - accuracy: 0.8525 - val_loss: 2.2347 - val_accuracy: 0.1850\n",
      "Epoch 157/600\n",
      "800/800 [==============================] - 0s 291us/step - loss: 0.4646 - accuracy: 0.8388 - val_loss: 2.0171 - val_accuracy: 0.2350\n",
      "Epoch 158/600\n",
      "800/800 [==============================] - 0s 274us/step - loss: 0.5436 - accuracy: 0.8238 - val_loss: 2.1448 - val_accuracy: 0.1950\n",
      "Epoch 159/600\n",
      "800/800 [==============================] - 0s 267us/step - loss: 0.4929 - accuracy: 0.8500 - val_loss: 2.0320 - val_accuracy: 0.2000\n",
      "Epoch 160/600\n",
      "800/800 [==============================] - 0s 278us/step - loss: 0.5147 - accuracy: 0.8238 - val_loss: 2.0042 - val_accuracy: 0.2000\n",
      "Epoch 161/600\n",
      "800/800 [==============================] - 0s 289us/step - loss: 0.4673 - accuracy: 0.8550 - val_loss: 2.0958 - val_accuracy: 0.1950\n",
      "Epoch 162/600\n",
      "800/800 [==============================] - 0s 307us/step - loss: 0.4818 - accuracy: 0.8500 - val_loss: 2.0207 - val_accuracy: 0.2050\n",
      "Epoch 163/600\n",
      "800/800 [==============================] - 0s 274us/step - loss: 0.5111 - accuracy: 0.8213 - val_loss: 2.1186 - val_accuracy: 0.1800\n",
      "Epoch 164/600\n",
      "800/800 [==============================] - 0s 282us/step - loss: 0.5290 - accuracy: 0.8375 - val_loss: 2.0351 - val_accuracy: 0.2100\n",
      "Epoch 165/600\n",
      "800/800 [==============================] - 0s 276us/step - loss: 0.5241 - accuracy: 0.8125 - val_loss: 2.0364 - val_accuracy: 0.2050\n",
      "Epoch 166/600\n",
      "800/800 [==============================] - 0s 279us/step - loss: 0.4861 - accuracy: 0.8388 - val_loss: 2.0410 - val_accuracy: 0.2350\n",
      "Epoch 167/600\n",
      "800/800 [==============================] - 0s 279us/step - loss: 0.4904 - accuracy: 0.8475 - val_loss: 2.2253 - val_accuracy: 0.1800\n",
      "Epoch 168/600\n",
      "800/800 [==============================] - 0s 271us/step - loss: 0.5086 - accuracy: 0.8350 - val_loss: 2.0903 - val_accuracy: 0.2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 169/600\n",
      "800/800 [==============================] - 0s 285us/step - loss: 0.4429 - accuracy: 0.8662 - val_loss: 2.1470 - val_accuracy: 0.1850\n",
      "Epoch 170/600\n",
      "800/800 [==============================] - 0s 296us/step - loss: 0.4554 - accuracy: 0.8625 - val_loss: 2.2502 - val_accuracy: 0.1750\n",
      "Epoch 171/600\n",
      "800/800 [==============================] - 0s 293us/step - loss: 0.5138 - accuracy: 0.8338 - val_loss: 2.3025 - val_accuracy: 0.1750\n",
      "Epoch 172/600\n",
      "800/800 [==============================] - 0s 286us/step - loss: 0.5731 - accuracy: 0.8062 - val_loss: 2.1902 - val_accuracy: 0.1900\n",
      "Epoch 173/600\n",
      "800/800 [==============================] - 0s 271us/step - loss: 0.5020 - accuracy: 0.8138 - val_loss: 2.0848 - val_accuracy: 0.2000\n",
      "Epoch 174/600\n",
      "800/800 [==============================] - 0s 285us/step - loss: 0.5168 - accuracy: 0.8375 - val_loss: 2.1513 - val_accuracy: 0.1950\n",
      "Epoch 175/600\n",
      "800/800 [==============================] - 0s 273us/step - loss: 0.4658 - accuracy: 0.8612 - val_loss: 2.1318 - val_accuracy: 0.1950\n",
      "Epoch 176/600\n",
      "800/800 [==============================] - 0s 290us/step - loss: 0.4556 - accuracy: 0.8475 - val_loss: 2.0401 - val_accuracy: 0.2000\n",
      "Epoch 177/600\n",
      "800/800 [==============================] - 0s 291us/step - loss: 0.4937 - accuracy: 0.8338 - val_loss: 1.9745 - val_accuracy: 0.2050\n",
      "Epoch 178/600\n",
      "800/800 [==============================] - 0s 283us/step - loss: 0.5311 - accuracy: 0.8150 - val_loss: 2.0718 - val_accuracy: 0.1900\n",
      "Epoch 179/600\n",
      "800/800 [==============================] - 0s 295us/step - loss: 0.4979 - accuracy: 0.8462 - val_loss: 2.0660 - val_accuracy: 0.2000\n",
      "Epoch 180/600\n",
      "800/800 [==============================] - 0s 279us/step - loss: 0.4632 - accuracy: 0.8537 - val_loss: 2.2159 - val_accuracy: 0.1800\n",
      "Epoch 181/600\n",
      "800/800 [==============================] - 0s 296us/step - loss: 0.4663 - accuracy: 0.8550 - val_loss: 2.0903 - val_accuracy: 0.1950\n",
      "Epoch 182/600\n",
      "800/800 [==============================] - 0s 278us/step - loss: 0.5268 - accuracy: 0.8225 - val_loss: 2.1907 - val_accuracy: 0.1800\n",
      "Epoch 183/600\n",
      "800/800 [==============================] - 0s 295us/step - loss: 0.4681 - accuracy: 0.8363 - val_loss: 2.0814 - val_accuracy: 0.2050\n",
      "Epoch 184/600\n",
      "800/800 [==============================] - 0s 297us/step - loss: 0.4917 - accuracy: 0.8462 - val_loss: 2.1015 - val_accuracy: 0.2000\n",
      "Epoch 185/600\n",
      "800/800 [==============================] - 0s 290us/step - loss: 0.5374 - accuracy: 0.8112 - val_loss: 2.0046 - val_accuracy: 0.2200\n",
      "Epoch 186/600\n",
      "800/800 [==============================] - 0s 304us/step - loss: 0.4503 - accuracy: 0.8550 - val_loss: 2.0632 - val_accuracy: 0.2050\n",
      "Epoch 187/600\n",
      "800/800 [==============================] - 0s 286us/step - loss: 0.5282 - accuracy: 0.8300 - val_loss: 2.0667 - val_accuracy: 0.2050\n",
      "Epoch 188/600\n",
      "800/800 [==============================] - 0s 271us/step - loss: 0.4925 - accuracy: 0.8438 - val_loss: 2.1176 - val_accuracy: 0.2000\n",
      "Epoch 189/600\n",
      "800/800 [==============================] - 0s 288us/step - loss: 0.5064 - accuracy: 0.8438 - val_loss: 2.1814 - val_accuracy: 0.1800\n",
      "Epoch 190/600\n",
      "800/800 [==============================] - 0s 283us/step - loss: 0.4510 - accuracy: 0.8400 - val_loss: 2.2418 - val_accuracy: 0.1800\n",
      "Epoch 191/600\n",
      "800/800 [==============================] - 0s 298us/step - loss: 0.4752 - accuracy: 0.8425 - val_loss: 1.9632 - val_accuracy: 0.2200\n",
      "Epoch 192/600\n",
      "800/800 [==============================] - 0s 257us/step - loss: 0.5177 - accuracy: 0.8263 - val_loss: 2.1146 - val_accuracy: 0.2000\n",
      "Epoch 193/600\n",
      "800/800 [==============================] - 0s 275us/step - loss: 0.5278 - accuracy: 0.8338 - val_loss: 2.1386 - val_accuracy: 0.2000\n",
      "Epoch 194/600\n",
      "800/800 [==============================] - 0s 294us/step - loss: 0.4818 - accuracy: 0.8325 - val_loss: 2.1144 - val_accuracy: 0.2100\n",
      "Epoch 195/600\n",
      "800/800 [==============================] - 0s 275us/step - loss: 0.4953 - accuracy: 0.8363 - val_loss: 2.1537 - val_accuracy: 0.1850\n",
      "Epoch 196/600\n",
      "800/800 [==============================] - 0s 279us/step - loss: 0.4372 - accuracy: 0.8500 - val_loss: 2.1236 - val_accuracy: 0.1950\n",
      "Epoch 197/600\n",
      "800/800 [==============================] - 0s 284us/step - loss: 0.4483 - accuracy: 0.8600 - val_loss: 2.2125 - val_accuracy: 0.1850\n",
      "Epoch 198/600\n",
      "800/800 [==============================] - 0s 273us/step - loss: 0.5126 - accuracy: 0.8300 - val_loss: 2.1123 - val_accuracy: 0.2000\n",
      "Epoch 199/600\n",
      "800/800 [==============================] - 0s 281us/step - loss: 0.4903 - accuracy: 0.8388 - val_loss: 2.1375 - val_accuracy: 0.1950\n",
      "Epoch 200/600\n",
      "800/800 [==============================] - 0s 287us/step - loss: 0.4761 - accuracy: 0.8413 - val_loss: 2.1458 - val_accuracy: 0.1950\n",
      "Epoch 201/600\n",
      "800/800 [==============================] - 0s 289us/step - loss: 0.4747 - accuracy: 0.8500 - val_loss: 1.8672 - val_accuracy: 0.2400\n",
      "Epoch 202/600\n",
      "800/800 [==============================] - 0s 292us/step - loss: 0.4979 - accuracy: 0.8400 - val_loss: 2.1613 - val_accuracy: 0.1900\n",
      "Epoch 203/600\n",
      "800/800 [==============================] - 0s 285us/step - loss: 0.4514 - accuracy: 0.8500 - val_loss: 2.1167 - val_accuracy: 0.1900\n",
      "Epoch 204/600\n",
      "800/800 [==============================] - 0s 289us/step - loss: 0.5152 - accuracy: 0.8300 - val_loss: 2.1258 - val_accuracy: 0.2000\n",
      "Epoch 205/600\n",
      "800/800 [==============================] - 0s 302us/step - loss: 0.4736 - accuracy: 0.8400 - val_loss: 2.1315 - val_accuracy: 0.1950\n",
      "Epoch 206/600\n",
      "800/800 [==============================] - 0s 278us/step - loss: 0.4487 - accuracy: 0.8475 - val_loss: 2.0883 - val_accuracy: 0.2000\n",
      "Epoch 207/600\n",
      "800/800 [==============================] - 0s 288us/step - loss: 0.4835 - accuracy: 0.8575 - val_loss: 2.0690 - val_accuracy: 0.2150\n",
      "Epoch 208/600\n",
      "800/800 [==============================] - 0s 277us/step - loss: 0.4562 - accuracy: 0.8562 - val_loss: 1.9837 - val_accuracy: 0.2350\n",
      "Epoch 209/600\n",
      "800/800 [==============================] - 0s 277us/step - loss: 0.4500 - accuracy: 0.8612 - val_loss: 1.9973 - val_accuracy: 0.2350\n",
      "Epoch 210/600\n",
      "800/800 [==============================] - 0s 281us/step - loss: 0.5074 - accuracy: 0.8275 - val_loss: 2.1433 - val_accuracy: 0.2050\n",
      "Epoch 211/600\n",
      "800/800 [==============================] - 0s 259us/step - loss: 0.4475 - accuracy: 0.8500 - val_loss: 2.0467 - val_accuracy: 0.2050\n",
      "Epoch 212/600\n",
      "800/800 [==============================] - 0s 271us/step - loss: 0.4768 - accuracy: 0.8512 - val_loss: 2.0168 - val_accuracy: 0.2100\n",
      "Epoch 213/600\n",
      "800/800 [==============================] - 0s 289us/step - loss: 0.4478 - accuracy: 0.8475 - val_loss: 2.3094 - val_accuracy: 0.1750\n",
      "Epoch 214/600\n",
      "800/800 [==============================] - 0s 276us/step - loss: 0.4765 - accuracy: 0.8388 - val_loss: 2.1764 - val_accuracy: 0.1950\n",
      "Epoch 215/600\n",
      "800/800 [==============================] - 0s 264us/step - loss: 0.4679 - accuracy: 0.8625 - val_loss: 2.2577 - val_accuracy: 0.1850\n",
      "Epoch 216/600\n",
      "800/800 [==============================] - 0s 292us/step - loss: 0.4481 - accuracy: 0.8537 - val_loss: 2.1129 - val_accuracy: 0.2000\n",
      "Epoch 217/600\n",
      "800/800 [==============================] - 0s 292us/step - loss: 0.4641 - accuracy: 0.8512 - val_loss: 2.2375 - val_accuracy: 0.1700\n",
      "Epoch 218/600\n",
      "800/800 [==============================] - 0s 275us/step - loss: 0.4925 - accuracy: 0.8400 - val_loss: 2.0987 - val_accuracy: 0.1950\n",
      "Epoch 219/600\n",
      "800/800 [==============================] - 0s 290us/step - loss: 0.4942 - accuracy: 0.8288 - val_loss: 2.0800 - val_accuracy: 0.2000\n",
      "Epoch 220/600\n",
      "800/800 [==============================] - 0s 276us/step - loss: 0.5015 - accuracy: 0.8350 - val_loss: 2.1762 - val_accuracy: 0.1950\n",
      "Epoch 221/600\n",
      "800/800 [==============================] - 0s 286us/step - loss: 0.4718 - accuracy: 0.8338 - val_loss: 2.1499 - val_accuracy: 0.1750\n",
      "Epoch 222/600\n",
      "800/800 [==============================] - 0s 264us/step - loss: 0.4327 - accuracy: 0.8612 - val_loss: 2.1094 - val_accuracy: 0.2050\n",
      "Epoch 223/600\n",
      "800/800 [==============================] - 0s 278us/step - loss: 0.4564 - accuracy: 0.8600 - val_loss: 2.1177 - val_accuracy: 0.1900\n",
      "Epoch 224/600\n",
      "800/800 [==============================] - 0s 293us/step - loss: 0.4652 - accuracy: 0.8575 - val_loss: 2.1513 - val_accuracy: 0.1850\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 225/600\n",
      "800/800 [==============================] - 0s 285us/step - loss: 0.4856 - accuracy: 0.8375 - val_loss: 2.1801 - val_accuracy: 0.1900\n",
      "Epoch 226/600\n",
      "800/800 [==============================] - 0s 290us/step - loss: 0.4883 - accuracy: 0.8450 - val_loss: 2.1401 - val_accuracy: 0.1950\n",
      "Epoch 227/600\n",
      "800/800 [==============================] - 0s 272us/step - loss: 0.4226 - accuracy: 0.8662 - val_loss: 2.0948 - val_accuracy: 0.2100\n",
      "Epoch 228/600\n",
      "800/800 [==============================] - 0s 292us/step - loss: 0.4911 - accuracy: 0.8450 - val_loss: 2.1934 - val_accuracy: 0.1850\n",
      "Epoch 229/600\n",
      "800/800 [==============================] - 0s 300us/step - loss: 0.4636 - accuracy: 0.8612 - val_loss: 2.2007 - val_accuracy: 0.1850\n",
      "Epoch 230/600\n",
      "800/800 [==============================] - 0s 283us/step - loss: 0.5172 - accuracy: 0.8288 - val_loss: 2.1366 - val_accuracy: 0.1900\n",
      "Epoch 231/600\n",
      "800/800 [==============================] - 0s 289us/step - loss: 0.4521 - accuracy: 0.8487 - val_loss: 2.0443 - val_accuracy: 0.2050\n",
      "Epoch 232/600\n",
      "800/800 [==============================] - 0s 277us/step - loss: 0.5250 - accuracy: 0.8275 - val_loss: 2.0634 - val_accuracy: 0.2050\n",
      "Epoch 233/600\n",
      "800/800 [==============================] - 0s 282us/step - loss: 0.5018 - accuracy: 0.8338 - val_loss: 2.1219 - val_accuracy: 0.1900\n",
      "Epoch 234/600\n",
      "800/800 [==============================] - 0s 283us/step - loss: 0.4028 - accuracy: 0.8625 - val_loss: 1.9948 - val_accuracy: 0.2250\n",
      "Epoch 235/600\n",
      "800/800 [==============================] - 0s 260us/step - loss: 0.4832 - accuracy: 0.8400 - val_loss: 2.0578 - val_accuracy: 0.1950\n",
      "Epoch 236/600\n",
      "800/800 [==============================] - 0s 276us/step - loss: 0.4757 - accuracy: 0.8413 - val_loss: 1.9939 - val_accuracy: 0.2100\n",
      "Epoch 237/600\n",
      "800/800 [==============================] - 0s 293us/step - loss: 0.4694 - accuracy: 0.8587 - val_loss: 2.2958 - val_accuracy: 0.1800\n",
      "Epoch 238/600\n",
      "800/800 [==============================] - 0s 289us/step - loss: 0.4908 - accuracy: 0.8338 - val_loss: 2.0709 - val_accuracy: 0.2050\n",
      "Epoch 239/600\n",
      "800/800 [==============================] - 0s 293us/step - loss: 0.4949 - accuracy: 0.8225 - val_loss: 1.9802 - val_accuracy: 0.2200\n",
      "Epoch 240/600\n",
      "800/800 [==============================] - 0s 282us/step - loss: 0.4931 - accuracy: 0.8250 - val_loss: 2.0964 - val_accuracy: 0.2100\n",
      "Epoch 241/600\n",
      "800/800 [==============================] - 0s 295us/step - loss: 0.5137 - accuracy: 0.8163 - val_loss: 2.3243 - val_accuracy: 0.1800\n",
      "Epoch 242/600\n",
      "800/800 [==============================] - 0s 269us/step - loss: 0.4657 - accuracy: 0.8438 - val_loss: 2.0967 - val_accuracy: 0.2000\n",
      "Epoch 243/600\n",
      "800/800 [==============================] - 0s 288us/step - loss: 0.4901 - accuracy: 0.8413 - val_loss: 2.2827 - val_accuracy: 0.1750\n",
      "Epoch 244/600\n",
      "800/800 [==============================] - 0s 290us/step - loss: 0.4789 - accuracy: 0.8425 - val_loss: 2.0819 - val_accuracy: 0.1900\n",
      "Epoch 245/600\n",
      "800/800 [==============================] - 0s 289us/step - loss: 0.4265 - accuracy: 0.8650 - val_loss: 2.1710 - val_accuracy: 0.1950\n",
      "Epoch 246/600\n",
      "800/800 [==============================] - 0s 273us/step - loss: 0.4556 - accuracy: 0.8675 - val_loss: 1.9864 - val_accuracy: 0.2100\n",
      "Epoch 247/600\n",
      "800/800 [==============================] - 0s 284us/step - loss: 0.4918 - accuracy: 0.8487 - val_loss: 2.1342 - val_accuracy: 0.1950\n",
      "Epoch 248/600\n",
      "800/800 [==============================] - 0s 253us/step - loss: 0.4668 - accuracy: 0.8475 - val_loss: 2.3368 - val_accuracy: 0.1750\n",
      "Epoch 249/600\n",
      "800/800 [==============================] - 0s 265us/step - loss: 0.4478 - accuracy: 0.8600 - val_loss: 1.8847 - val_accuracy: 0.2250\n",
      "Epoch 250/600\n",
      "800/800 [==============================] - 0s 278us/step - loss: 0.4775 - accuracy: 0.8413 - val_loss: 2.3027 - val_accuracy: 0.1750\n",
      "Epoch 251/600\n",
      "800/800 [==============================] - 0s 265us/step - loss: 0.5149 - accuracy: 0.8313 - val_loss: 2.2548 - val_accuracy: 0.1800\n",
      "Epoch 252/600\n",
      "800/800 [==============================] - 0s 292us/step - loss: 0.4678 - accuracy: 0.8425 - val_loss: 2.2253 - val_accuracy: 0.1900\n",
      "Epoch 253/600\n",
      "800/800 [==============================] - 0s 270us/step - loss: 0.4330 - accuracy: 0.8725 - val_loss: 2.2152 - val_accuracy: 0.2000\n",
      "Epoch 254/600\n",
      "800/800 [==============================] - 0s 284us/step - loss: 0.5198 - accuracy: 0.8375 - val_loss: 2.0966 - val_accuracy: 0.2250\n",
      "Epoch 255/600\n",
      "800/800 [==============================] - 0s 265us/step - loss: 0.4792 - accuracy: 0.8500 - val_loss: 2.1236 - val_accuracy: 0.2200\n",
      "Epoch 256/600\n",
      "800/800 [==============================] - 0s 280us/step - loss: 0.4986 - accuracy: 0.8313 - val_loss: 1.8658 - val_accuracy: 0.2350\n",
      "Epoch 257/600\n",
      "800/800 [==============================] - 0s 283us/step - loss: 0.4452 - accuracy: 0.8500 - val_loss: 2.2070 - val_accuracy: 0.1900\n",
      "Epoch 258/600\n",
      "800/800 [==============================] - 0s 284us/step - loss: 0.4910 - accuracy: 0.8388 - val_loss: 2.1900 - val_accuracy: 0.1950\n",
      "Epoch 259/600\n",
      "800/800 [==============================] - 0s 272us/step - loss: 0.4650 - accuracy: 0.8388 - val_loss: 2.2307 - val_accuracy: 0.1850\n",
      "Epoch 260/600\n",
      "800/800 [==============================] - 0s 282us/step - loss: 0.4703 - accuracy: 0.8450 - val_loss: 2.1645 - val_accuracy: 0.1900\n",
      "Epoch 261/600\n",
      "800/800 [==============================] - 0s 288us/step - loss: 0.4219 - accuracy: 0.8675 - val_loss: 2.0620 - val_accuracy: 0.2100\n",
      "Epoch 262/600\n",
      "800/800 [==============================] - 0s 305us/step - loss: 0.4711 - accuracy: 0.8512 - val_loss: 2.1400 - val_accuracy: 0.1900\n",
      "Epoch 263/600\n",
      "800/800 [==============================] - 0s 291us/step - loss: 0.4229 - accuracy: 0.8637 - val_loss: 2.2826 - val_accuracy: 0.1900\n",
      "Epoch 264/600\n",
      "800/800 [==============================] - 0s 277us/step - loss: 0.4913 - accuracy: 0.8575 - val_loss: 2.1478 - val_accuracy: 0.2100\n",
      "Epoch 265/600\n",
      "800/800 [==============================] - 0s 287us/step - loss: 0.4259 - accuracy: 0.8363 - val_loss: 2.2360 - val_accuracy: 0.1950\n",
      "Epoch 266/600\n",
      "800/800 [==============================] - 0s 296us/step - loss: 0.4561 - accuracy: 0.8600 - val_loss: 2.1868 - val_accuracy: 0.2050\n",
      "Epoch 267/600\n",
      "800/800 [==============================] - 0s 290us/step - loss: 0.4866 - accuracy: 0.8350 - val_loss: 2.2012 - val_accuracy: 0.2000\n",
      "Epoch 268/600\n",
      "800/800 [==============================] - 0s 292us/step - loss: 0.5109 - accuracy: 0.8363 - val_loss: 2.1800 - val_accuracy: 0.1950\n",
      "Epoch 269/600\n",
      "800/800 [==============================] - 0s 297us/step - loss: 0.4185 - accuracy: 0.8612 - val_loss: 2.2118 - val_accuracy: 0.2050\n",
      "Epoch 270/600\n",
      "800/800 [==============================] - 0s 293us/step - loss: 0.4364 - accuracy: 0.8512 - val_loss: 2.1920 - val_accuracy: 0.1950\n",
      "Epoch 271/600\n",
      "800/800 [==============================] - 0s 300us/step - loss: 0.4783 - accuracy: 0.8537 - val_loss: 2.1492 - val_accuracy: 0.2050\n",
      "Epoch 272/600\n",
      "800/800 [==============================] - 0s 273us/step - loss: 0.4920 - accuracy: 0.8375 - val_loss: 2.2492 - val_accuracy: 0.1950\n",
      "Epoch 273/600\n",
      "800/800 [==============================] - 0s 269us/step - loss: 0.4868 - accuracy: 0.8537 - val_loss: 2.0773 - val_accuracy: 0.2050\n",
      "Epoch 274/600\n",
      "800/800 [==============================] - 0s 287us/step - loss: 0.4819 - accuracy: 0.8375 - val_loss: 2.1633 - val_accuracy: 0.2050\n",
      "Epoch 275/600\n",
      "800/800 [==============================] - 0s 284us/step - loss: 0.4616 - accuracy: 0.8462 - val_loss: 2.0825 - val_accuracy: 0.2250\n",
      "Epoch 276/600\n",
      "800/800 [==============================] - 0s 276us/step - loss: 0.4547 - accuracy: 0.8562 - val_loss: 2.1900 - val_accuracy: 0.2000\n",
      "Epoch 277/600\n",
      "800/800 [==============================] - 0s 258us/step - loss: 0.4812 - accuracy: 0.8500 - val_loss: 2.2369 - val_accuracy: 0.2000\n",
      "Epoch 278/600\n",
      "800/800 [==============================] - 0s 279us/step - loss: 0.4216 - accuracy: 0.8662 - val_loss: 2.1844 - val_accuracy: 0.2050\n",
      "Epoch 279/600\n",
      "800/800 [==============================] - 0s 286us/step - loss: 0.4540 - accuracy: 0.8537 - val_loss: 2.2866 - val_accuracy: 0.1850\n",
      "Epoch 280/600\n",
      "800/800 [==============================] - 0s 297us/step - loss: 0.4697 - accuracy: 0.8413 - val_loss: 2.1399 - val_accuracy: 0.2100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 281/600\n",
      "800/800 [==============================] - 0s 285us/step - loss: 0.5149 - accuracy: 0.8388 - val_loss: 2.2403 - val_accuracy: 0.2050\n",
      "Epoch 282/600\n",
      "800/800 [==============================] - 0s 290us/step - loss: 0.4691 - accuracy: 0.8487 - val_loss: 2.3066 - val_accuracy: 0.1950\n",
      "Epoch 283/600\n",
      "800/800 [==============================] - 0s 295us/step - loss: 0.4367 - accuracy: 0.8587 - val_loss: 2.2292 - val_accuracy: 0.1950\n",
      "Epoch 284/600\n",
      "800/800 [==============================] - 0s 283us/step - loss: 0.4440 - accuracy: 0.8587 - val_loss: 2.1443 - val_accuracy: 0.2100\n",
      "Epoch 285/600\n",
      "800/800 [==============================] - 0s 288us/step - loss: 0.4796 - accuracy: 0.8487 - val_loss: 2.2603 - val_accuracy: 0.2000\n",
      "Epoch 286/600\n",
      "800/800 [==============================] - 0s 276us/step - loss: 0.4522 - accuracy: 0.8475 - val_loss: 2.1932 - val_accuracy: 0.2000\n",
      "Epoch 287/600\n",
      "800/800 [==============================] - 0s 284us/step - loss: 0.4863 - accuracy: 0.8388 - val_loss: 1.9830 - val_accuracy: 0.2150\n",
      "Epoch 288/600\n",
      "800/800 [==============================] - 0s 288us/step - loss: 0.4492 - accuracy: 0.8512 - val_loss: 2.3775 - val_accuracy: 0.1750\n",
      "Epoch 289/600\n",
      "800/800 [==============================] - 0s 308us/step - loss: 0.4892 - accuracy: 0.8413 - val_loss: 2.3088 - val_accuracy: 0.1900\n",
      "Epoch 290/600\n",
      "800/800 [==============================] - 0s 267us/step - loss: 0.4471 - accuracy: 0.8562 - val_loss: 2.2645 - val_accuracy: 0.1900\n",
      "Epoch 291/600\n",
      "800/800 [==============================] - 0s 272us/step - loss: 0.4343 - accuracy: 0.8612 - val_loss: 1.9344 - val_accuracy: 0.2450\n",
      "Epoch 292/600\n",
      "800/800 [==============================] - 0s 300us/step - loss: 0.4429 - accuracy: 0.8462 - val_loss: 2.1420 - val_accuracy: 0.2100\n",
      "Epoch 293/600\n",
      "800/800 [==============================] - 0s 282us/step - loss: 0.4206 - accuracy: 0.8637 - val_loss: 2.2533 - val_accuracy: 0.1900\n",
      "Epoch 294/600\n",
      "800/800 [==============================] - 0s 271us/step - loss: 0.4659 - accuracy: 0.8413 - val_loss: 2.1778 - val_accuracy: 0.2100\n",
      "Epoch 295/600\n",
      "800/800 [==============================] - 0s 288us/step - loss: 0.4239 - accuracy: 0.8562 - val_loss: 2.3871 - val_accuracy: 0.1750\n",
      "Epoch 296/600\n",
      "800/800 [==============================] - 0s 261us/step - loss: 0.4449 - accuracy: 0.8600 - val_loss: 2.3439 - val_accuracy: 0.1850\n",
      "Epoch 297/600\n",
      "800/800 [==============================] - 0s 298us/step - loss: 0.4847 - accuracy: 0.8375 - val_loss: 2.2750 - val_accuracy: 0.1900\n",
      "Epoch 298/600\n",
      "800/800 [==============================] - 0s 284us/step - loss: 0.4200 - accuracy: 0.8737 - val_loss: 2.2010 - val_accuracy: 0.1950\n",
      "Epoch 299/600\n",
      "800/800 [==============================] - 0s 276us/step - loss: 0.4312 - accuracy: 0.8600 - val_loss: 2.2584 - val_accuracy: 0.1900\n",
      "Epoch 300/600\n",
      "800/800 [==============================] - 0s 270us/step - loss: 0.4442 - accuracy: 0.8525 - val_loss: 2.0454 - val_accuracy: 0.2050\n",
      "Epoch 301/600\n",
      "800/800 [==============================] - 0s 304us/step - loss: 0.4800 - accuracy: 0.8375 - val_loss: 2.2429 - val_accuracy: 0.1850\n",
      "Epoch 302/600\n",
      "800/800 [==============================] - 0s 291us/step - loss: 0.4713 - accuracy: 0.8413 - val_loss: 2.1198 - val_accuracy: 0.2150\n",
      "Epoch 303/600\n",
      "800/800 [==============================] - 0s 282us/step - loss: 0.5002 - accuracy: 0.8375 - val_loss: 2.0907 - val_accuracy: 0.2200\n",
      "Epoch 304/600\n",
      "800/800 [==============================] - 0s 289us/step - loss: 0.4509 - accuracy: 0.8525 - val_loss: 2.2034 - val_accuracy: 0.2050\n",
      "Epoch 305/600\n",
      "800/800 [==============================] - 0s 287us/step - loss: 0.4678 - accuracy: 0.8363 - val_loss: 2.3063 - val_accuracy: 0.1900\n",
      "Epoch 306/600\n",
      "800/800 [==============================] - 0s 301us/step - loss: 0.4935 - accuracy: 0.8288 - val_loss: 2.1988 - val_accuracy: 0.1950\n",
      "Epoch 307/600\n",
      "800/800 [==============================] - 0s 285us/step - loss: 0.4487 - accuracy: 0.8575 - val_loss: 2.2855 - val_accuracy: 0.1900\n",
      "Epoch 308/600\n",
      "800/800 [==============================] - 0s 276us/step - loss: 0.4850 - accuracy: 0.8400 - val_loss: 2.1581 - val_accuracy: 0.2050\n",
      "Epoch 309/600\n",
      "800/800 [==============================] - 0s 291us/step - loss: 0.4490 - accuracy: 0.8525 - val_loss: 2.3136 - val_accuracy: 0.2000\n",
      "Epoch 310/600\n",
      "800/800 [==============================] - 0s 296us/step - loss: 0.4566 - accuracy: 0.8462 - val_loss: 2.2119 - val_accuracy: 0.2050\n",
      "Epoch 311/600\n",
      "800/800 [==============================] - 0s 291us/step - loss: 0.4537 - accuracy: 0.8600 - val_loss: 2.0560 - val_accuracy: 0.2200\n",
      "Epoch 312/600\n",
      "800/800 [==============================] - 0s 260us/step - loss: 0.4186 - accuracy: 0.8687 - val_loss: 2.2463 - val_accuracy: 0.2100\n",
      "Epoch 313/600\n",
      "800/800 [==============================] - 0s 273us/step - loss: 0.4481 - accuracy: 0.8550 - val_loss: 2.2612 - val_accuracy: 0.2100\n",
      "Epoch 314/600\n",
      "800/800 [==============================] - 0s 253us/step - loss: 0.5033 - accuracy: 0.8313 - val_loss: 2.1273 - val_accuracy: 0.2250\n",
      "Epoch 315/600\n",
      "800/800 [==============================] - 0s 282us/step - loss: 0.4766 - accuracy: 0.8375 - val_loss: 2.3552 - val_accuracy: 0.1900\n",
      "Epoch 316/600\n",
      "800/800 [==============================] - 0s 296us/step - loss: 0.4532 - accuracy: 0.8637 - val_loss: 2.1632 - val_accuracy: 0.2050\n",
      "Epoch 317/600\n",
      "800/800 [==============================] - 0s 288us/step - loss: 0.4711 - accuracy: 0.8350 - val_loss: 2.2079 - val_accuracy: 0.2000\n",
      "Epoch 318/600\n",
      "800/800 [==============================] - 0s 287us/step - loss: 0.4684 - accuracy: 0.8438 - val_loss: 2.1736 - val_accuracy: 0.2000\n",
      "Epoch 319/600\n",
      "800/800 [==============================] - 0s 279us/step - loss: 0.4434 - accuracy: 0.8500 - val_loss: 2.2706 - val_accuracy: 0.1950\n",
      "Epoch 320/600\n",
      "800/800 [==============================] - 0s 266us/step - loss: 0.5034 - accuracy: 0.8263 - val_loss: 2.2344 - val_accuracy: 0.2050\n",
      "Epoch 321/600\n",
      "800/800 [==============================] - 0s 261us/step - loss: 0.4308 - accuracy: 0.8650 - val_loss: 2.0042 - val_accuracy: 0.2400\n",
      "Epoch 322/600\n",
      "800/800 [==============================] - 0s 269us/step - loss: 0.4949 - accuracy: 0.8175 - val_loss: 2.2623 - val_accuracy: 0.2000\n",
      "Epoch 323/600\n",
      "800/800 [==============================] - 0s 264us/step - loss: 0.5040 - accuracy: 0.8350 - val_loss: 2.2315 - val_accuracy: 0.1950\n",
      "Epoch 324/600\n",
      "800/800 [==============================] - 0s 292us/step - loss: 0.4596 - accuracy: 0.8487 - val_loss: 2.0292 - val_accuracy: 0.2200\n",
      "Epoch 325/600\n",
      "800/800 [==============================] - 0s 301us/step - loss: 0.4464 - accuracy: 0.8462 - val_loss: 2.2182 - val_accuracy: 0.2100\n",
      "Epoch 326/600\n",
      "800/800 [==============================] - 0s 296us/step - loss: 0.4210 - accuracy: 0.8675 - val_loss: 2.2170 - val_accuracy: 0.2100\n",
      "Epoch 327/600\n",
      "800/800 [==============================] - 0s 278us/step - loss: 0.4627 - accuracy: 0.8500 - val_loss: 2.3376 - val_accuracy: 0.1800\n",
      "Epoch 328/600\n",
      "800/800 [==============================] - 0s 289us/step - loss: 0.4915 - accuracy: 0.8425 - val_loss: 2.1300 - val_accuracy: 0.2100\n",
      "Epoch 329/600\n",
      "800/800 [==============================] - 0s 305us/step - loss: 0.4640 - accuracy: 0.8525 - val_loss: 2.2279 - val_accuracy: 0.1950\n",
      "Epoch 330/600\n",
      "800/800 [==============================] - 0s 290us/step - loss: 0.5067 - accuracy: 0.8375 - val_loss: 2.3272 - val_accuracy: 0.1950\n",
      "Epoch 331/600\n",
      "800/800 [==============================] - 0s 294us/step - loss: 0.4463 - accuracy: 0.8650 - val_loss: 2.2467 - val_accuracy: 0.2050\n",
      "Epoch 332/600\n",
      "800/800 [==============================] - 0s 289us/step - loss: 0.4437 - accuracy: 0.8575 - val_loss: 2.4072 - val_accuracy: 0.1850\n",
      "Epoch 333/600\n",
      "800/800 [==============================] - 0s 288us/step - loss: 0.4441 - accuracy: 0.8662 - val_loss: 2.2029 - val_accuracy: 0.2000\n",
      "Epoch 334/600\n",
      "800/800 [==============================] - 0s 277us/step - loss: 0.4381 - accuracy: 0.8650 - val_loss: 2.1720 - val_accuracy: 0.2000\n",
      "Epoch 335/600\n",
      "800/800 [==============================] - 0s 281us/step - loss: 0.4748 - accuracy: 0.8438 - val_loss: 2.3079 - val_accuracy: 0.1950\n",
      "Epoch 336/600\n",
      "800/800 [==============================] - 0s 273us/step - loss: 0.4889 - accuracy: 0.8388 - val_loss: 2.1775 - val_accuracy: 0.2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 337/600\n",
      "800/800 [==============================] - 0s 276us/step - loss: 0.4497 - accuracy: 0.8500 - val_loss: 2.2564 - val_accuracy: 0.1800\n",
      "Epoch 338/600\n",
      "800/800 [==============================] - 0s 266us/step - loss: 0.4753 - accuracy: 0.8525 - val_loss: 2.2585 - val_accuracy: 0.1950\n",
      "Epoch 339/600\n",
      "800/800 [==============================] - 0s 285us/step - loss: 0.4103 - accuracy: 0.8537 - val_loss: 2.1831 - val_accuracy: 0.1900\n",
      "Epoch 340/600\n",
      "800/800 [==============================] - 0s 269us/step - loss: 0.4680 - accuracy: 0.8400 - val_loss: 2.3173 - val_accuracy: 0.1850\n",
      "Epoch 341/600\n",
      "800/800 [==============================] - 0s 288us/step - loss: 0.4520 - accuracy: 0.8562 - val_loss: 2.5065 - val_accuracy: 0.1850\n",
      "Epoch 342/600\n",
      "800/800 [==============================] - 0s 289us/step - loss: 0.4686 - accuracy: 0.8450 - val_loss: 2.4365 - val_accuracy: 0.1850\n",
      "Epoch 343/600\n",
      "800/800 [==============================] - 0s 284us/step - loss: 0.4802 - accuracy: 0.8413 - val_loss: 2.2850 - val_accuracy: 0.1900\n",
      "Epoch 344/600\n",
      "800/800 [==============================] - 0s 317us/step - loss: 0.4194 - accuracy: 0.8612 - val_loss: 2.2436 - val_accuracy: 0.1900\n",
      "Epoch 345/600\n",
      "800/800 [==============================] - 0s 295us/step - loss: 0.4469 - accuracy: 0.8637 - val_loss: 2.3596 - val_accuracy: 0.1900\n",
      "Epoch 346/600\n",
      "800/800 [==============================] - 0s 297us/step - loss: 0.4161 - accuracy: 0.8775 - val_loss: 2.2688 - val_accuracy: 0.1950\n",
      "Epoch 347/600\n",
      "800/800 [==============================] - 0s 286us/step - loss: 0.4773 - accuracy: 0.8325 - val_loss: 2.1202 - val_accuracy: 0.2300\n",
      "Epoch 348/600\n",
      "800/800 [==============================] - 0s 271us/step - loss: 0.4697 - accuracy: 0.8475 - val_loss: 2.4732 - val_accuracy: 0.1950\n",
      "Epoch 349/600\n",
      "800/800 [==============================] - 0s 279us/step - loss: 0.5042 - accuracy: 0.8288 - val_loss: 2.3304 - val_accuracy: 0.2050\n",
      "Epoch 350/600\n",
      "800/800 [==============================] - 0s 272us/step - loss: 0.4768 - accuracy: 0.8438 - val_loss: 2.2899 - val_accuracy: 0.1950\n",
      "Epoch 351/600\n",
      "800/800 [==============================] - 0s 299us/step - loss: 0.5064 - accuracy: 0.8288 - val_loss: 2.3381 - val_accuracy: 0.1900\n",
      "Epoch 352/600\n",
      "800/800 [==============================] - 0s 276us/step - loss: 0.4634 - accuracy: 0.8475 - val_loss: 2.2568 - val_accuracy: 0.2000\n",
      "Epoch 353/600\n",
      "800/800 [==============================] - 0s 275us/step - loss: 0.4384 - accuracy: 0.8512 - val_loss: 2.3049 - val_accuracy: 0.2000\n",
      "Epoch 354/600\n",
      "800/800 [==============================] - 0s 283us/step - loss: 0.4529 - accuracy: 0.8487 - val_loss: 2.4749 - val_accuracy: 0.1850\n",
      "Epoch 355/600\n",
      "800/800 [==============================] - 0s 301us/step - loss: 0.3898 - accuracy: 0.8712 - val_loss: 2.1835 - val_accuracy: 0.2100\n",
      "Epoch 356/600\n",
      "800/800 [==============================] - 0s 301us/step - loss: 0.4079 - accuracy: 0.8712 - val_loss: 2.7211 - val_accuracy: 0.1650\n",
      "Epoch 357/600\n",
      "800/800 [==============================] - 0s 282us/step - loss: 0.4693 - accuracy: 0.8500 - val_loss: 2.4158 - val_accuracy: 0.1800\n",
      "Epoch 358/600\n",
      "800/800 [==============================] - 0s 280us/step - loss: 0.4619 - accuracy: 0.8475 - val_loss: 2.3119 - val_accuracy: 0.1950\n",
      "Epoch 359/600\n",
      "800/800 [==============================] - 0s 290us/step - loss: 0.4498 - accuracy: 0.8500 - val_loss: 2.2488 - val_accuracy: 0.2000\n",
      "Epoch 360/600\n",
      "800/800 [==============================] - 0s 285us/step - loss: 0.4874 - accuracy: 0.8500 - val_loss: 2.5085 - val_accuracy: 0.1750\n",
      "Epoch 361/600\n",
      "800/800 [==============================] - 0s 271us/step - loss: 0.4255 - accuracy: 0.8625 - val_loss: 2.3367 - val_accuracy: 0.1900\n",
      "Epoch 362/600\n",
      "800/800 [==============================] - 0s 297us/step - loss: 0.4398 - accuracy: 0.8512 - val_loss: 2.2986 - val_accuracy: 0.1950\n",
      "Epoch 363/600\n",
      "800/800 [==============================] - 0s 273us/step - loss: 0.4269 - accuracy: 0.8650 - val_loss: 2.2424 - val_accuracy: 0.2050\n",
      "Epoch 364/600\n",
      "800/800 [==============================] - 0s 290us/step - loss: 0.3945 - accuracy: 0.8763 - val_loss: 2.3309 - val_accuracy: 0.1950\n",
      "Epoch 365/600\n",
      "800/800 [==============================] - 0s 269us/step - loss: 0.4177 - accuracy: 0.8700 - val_loss: 2.3063 - val_accuracy: 0.2000\n",
      "Epoch 366/600\n",
      "800/800 [==============================] - 0s 271us/step - loss: 0.4443 - accuracy: 0.8450 - val_loss: 2.4791 - val_accuracy: 0.1900\n",
      "Epoch 367/600\n",
      "800/800 [==============================] - 0s 288us/step - loss: 0.4686 - accuracy: 0.8512 - val_loss: 2.1750 - val_accuracy: 0.2200\n",
      "Epoch 368/600\n",
      "800/800 [==============================] - 0s 282us/step - loss: 0.4598 - accuracy: 0.8350 - val_loss: 2.0082 - val_accuracy: 0.2250\n",
      "Epoch 369/600\n",
      "800/800 [==============================] - 0s 280us/step - loss: 0.4450 - accuracy: 0.8462 - val_loss: 2.3939 - val_accuracy: 0.1900\n",
      "Epoch 370/600\n",
      "800/800 [==============================] - 0s 292us/step - loss: 0.4725 - accuracy: 0.8438 - val_loss: 2.3272 - val_accuracy: 0.2000\n",
      "Epoch 371/600\n",
      "800/800 [==============================] - 0s 280us/step - loss: 0.4162 - accuracy: 0.8562 - val_loss: 2.2321 - val_accuracy: 0.2100\n",
      "Epoch 372/600\n",
      "800/800 [==============================] - 0s 285us/step - loss: 0.4218 - accuracy: 0.8600 - val_loss: 2.2947 - val_accuracy: 0.2000\n",
      "Epoch 373/600\n",
      "800/800 [==============================] - 0s 271us/step - loss: 0.4929 - accuracy: 0.8338 - val_loss: 2.2912 - val_accuracy: 0.2000\n",
      "Epoch 374/600\n",
      "800/800 [==============================] - 0s 275us/step - loss: 0.4427 - accuracy: 0.8550 - val_loss: 2.1213 - val_accuracy: 0.2050\n",
      "Epoch 375/600\n",
      "800/800 [==============================] - 0s 276us/step - loss: 0.4451 - accuracy: 0.8500 - val_loss: 2.4025 - val_accuracy: 0.1950\n",
      "Epoch 376/600\n",
      "800/800 [==============================] - 0s 279us/step - loss: 0.4609 - accuracy: 0.8500 - val_loss: 2.1619 - val_accuracy: 0.2000\n",
      "Epoch 377/600\n",
      "800/800 [==============================] - 0s 298us/step - loss: 0.4212 - accuracy: 0.8587 - val_loss: 2.2077 - val_accuracy: 0.2000\n",
      "Epoch 378/600\n",
      "800/800 [==============================] - 0s 298us/step - loss: 0.4598 - accuracy: 0.8462 - val_loss: 2.2683 - val_accuracy: 0.2000\n",
      "Epoch 379/600\n",
      "800/800 [==============================] - 0s 294us/step - loss: 0.4450 - accuracy: 0.8438 - val_loss: 2.2660 - val_accuracy: 0.2000\n",
      "Epoch 380/600\n",
      "800/800 [==============================] - 0s 322us/step - loss: 0.4693 - accuracy: 0.8625 - val_loss: 2.3468 - val_accuracy: 0.2000\n",
      "Epoch 381/600\n",
      "800/800 [==============================] - 0s 286us/step - loss: 0.4648 - accuracy: 0.8550 - val_loss: 2.1306 - val_accuracy: 0.2250\n",
      "Epoch 382/600\n",
      "800/800 [==============================] - 0s 275us/step - loss: 0.4260 - accuracy: 0.8575 - val_loss: 2.1249 - val_accuracy: 0.2450\n",
      "Epoch 383/600\n",
      "800/800 [==============================] - 0s 268us/step - loss: 0.4294 - accuracy: 0.8675 - val_loss: 2.1523 - val_accuracy: 0.2200\n",
      "Epoch 384/600\n",
      "800/800 [==============================] - 0s 295us/step - loss: 0.4651 - accuracy: 0.8388 - val_loss: 2.3167 - val_accuracy: 0.2100\n",
      "Epoch 385/600\n",
      "800/800 [==============================] - 0s 274us/step - loss: 0.4663 - accuracy: 0.8525 - val_loss: 2.2036 - val_accuracy: 0.2050\n",
      "Epoch 386/600\n",
      "800/800 [==============================] - 0s 292us/step - loss: 0.4461 - accuracy: 0.8525 - val_loss: 2.2897 - val_accuracy: 0.2150\n",
      "Epoch 387/600\n",
      "800/800 [==============================] - 0s 313us/step - loss: 0.4937 - accuracy: 0.8275 - val_loss: 2.3216 - val_accuracy: 0.1950\n",
      "Epoch 388/600\n",
      "800/800 [==============================] - 0s 310us/step - loss: 0.4441 - accuracy: 0.8462 - val_loss: 2.4324 - val_accuracy: 0.1800\n",
      "Epoch 389/600\n",
      "800/800 [==============================] - 0s 245us/step - loss: 0.4298 - accuracy: 0.8537 - val_loss: 2.1685 - val_accuracy: 0.2450\n",
      "Epoch 390/600\n",
      "800/800 [==============================] - 0s 284us/step - loss: 0.4143 - accuracy: 0.8662 - val_loss: 2.3863 - val_accuracy: 0.1900\n",
      "Epoch 391/600\n",
      "800/800 [==============================] - 0s 285us/step - loss: 0.4129 - accuracy: 0.8662 - val_loss: 2.2350 - val_accuracy: 0.2000\n",
      "Epoch 392/600\n",
      "800/800 [==============================] - 0s 298us/step - loss: 0.4313 - accuracy: 0.8600 - val_loss: 2.2307 - val_accuracy: 0.2300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 393/600\n",
      "800/800 [==============================] - 0s 292us/step - loss: 0.4348 - accuracy: 0.8575 - val_loss: 2.2423 - val_accuracy: 0.1950\n",
      "Epoch 394/600\n",
      "800/800 [==============================] - 0s 273us/step - loss: 0.4419 - accuracy: 0.8512 - val_loss: 2.2387 - val_accuracy: 0.1900\n",
      "Epoch 395/600\n",
      "800/800 [==============================] - 0s 302us/step - loss: 0.4325 - accuracy: 0.8512 - val_loss: 2.3330 - val_accuracy: 0.1950\n",
      "Epoch 396/600\n",
      "800/800 [==============================] - 0s 295us/step - loss: 0.4822 - accuracy: 0.8388 - val_loss: 2.4373 - val_accuracy: 0.1950\n",
      "Epoch 397/600\n",
      "800/800 [==============================] - 0s 281us/step - loss: 0.4370 - accuracy: 0.8537 - val_loss: 2.4855 - val_accuracy: 0.1950\n",
      "Epoch 398/600\n",
      "800/800 [==============================] - 0s 289us/step - loss: 0.4419 - accuracy: 0.8575 - val_loss: 2.3319 - val_accuracy: 0.1900\n",
      "Epoch 399/600\n",
      "800/800 [==============================] - 0s 285us/step - loss: 0.4919 - accuracy: 0.8325 - val_loss: 2.1666 - val_accuracy: 0.2600\n",
      "Epoch 400/600\n",
      "800/800 [==============================] - 0s 263us/step - loss: 0.4339 - accuracy: 0.8587 - val_loss: 2.4114 - val_accuracy: 0.1900\n",
      "Epoch 401/600\n",
      "800/800 [==============================] - 0s 280us/step - loss: 0.3942 - accuracy: 0.8725 - val_loss: 2.2574 - val_accuracy: 0.2250\n",
      "Epoch 402/600\n",
      "800/800 [==============================] - 0s 281us/step - loss: 0.4362 - accuracy: 0.8612 - val_loss: 2.2465 - val_accuracy: 0.2100\n",
      "Epoch 403/600\n",
      "800/800 [==============================] - 0s 286us/step - loss: 0.3868 - accuracy: 0.8775 - val_loss: 2.3325 - val_accuracy: 0.2150\n",
      "Epoch 404/600\n",
      "800/800 [==============================] - 0s 301us/step - loss: 0.4611 - accuracy: 0.8462 - val_loss: 2.4074 - val_accuracy: 0.1900\n",
      "Epoch 405/600\n",
      "800/800 [==============================] - 0s 305us/step - loss: 0.4588 - accuracy: 0.8625 - val_loss: 2.4040 - val_accuracy: 0.1950\n",
      "Epoch 406/600\n",
      "800/800 [==============================] - 0s 290us/step - loss: 0.4107 - accuracy: 0.8562 - val_loss: 2.1791 - val_accuracy: 0.2350\n",
      "Epoch 407/600\n",
      "800/800 [==============================] - 0s 272us/step - loss: 0.4206 - accuracy: 0.8800 - val_loss: 2.3148 - val_accuracy: 0.2350\n",
      "Epoch 408/600\n",
      "800/800 [==============================] - 0s 282us/step - loss: 0.4712 - accuracy: 0.8450 - val_loss: 2.2711 - val_accuracy: 0.2250\n",
      "Epoch 409/600\n",
      "800/800 [==============================] - 0s 284us/step - loss: 0.4280 - accuracy: 0.8512 - val_loss: 2.3268 - val_accuracy: 0.1950\n",
      "Epoch 410/600\n",
      "800/800 [==============================] - 0s 279us/step - loss: 0.4656 - accuracy: 0.8400 - val_loss: 2.3335 - val_accuracy: 0.2050\n",
      "Epoch 411/600\n",
      "800/800 [==============================] - 0s 265us/step - loss: 0.4205 - accuracy: 0.8612 - val_loss: 2.2412 - val_accuracy: 0.2250\n",
      "Epoch 412/600\n",
      "800/800 [==============================] - 0s 267us/step - loss: 0.4182 - accuracy: 0.8425 - val_loss: 2.3609 - val_accuracy: 0.1900\n",
      "Epoch 413/600\n",
      "800/800 [==============================] - 0s 291us/step - loss: 0.4321 - accuracy: 0.8575 - val_loss: 2.3874 - val_accuracy: 0.2050\n",
      "Epoch 414/600\n",
      "800/800 [==============================] - 0s 272us/step - loss: 0.4466 - accuracy: 0.8525 - val_loss: 2.3392 - val_accuracy: 0.2100\n",
      "Epoch 415/600\n",
      "800/800 [==============================] - 0s 262us/step - loss: 0.4375 - accuracy: 0.8462 - val_loss: 2.2256 - val_accuracy: 0.2800\n",
      "Epoch 416/600\n",
      "800/800 [==============================] - 0s 283us/step - loss: 0.4353 - accuracy: 0.8575 - val_loss: 2.3547 - val_accuracy: 0.2100\n",
      "Epoch 417/600\n",
      "800/800 [==============================] - 0s 287us/step - loss: 0.4101 - accuracy: 0.8675 - val_loss: 2.4313 - val_accuracy: 0.2150\n",
      "Epoch 418/600\n",
      "800/800 [==============================] - 0s 286us/step - loss: 0.4419 - accuracy: 0.8550 - val_loss: 2.3077 - val_accuracy: 0.2450\n",
      "Epoch 419/600\n",
      "800/800 [==============================] - 0s 282us/step - loss: 0.3842 - accuracy: 0.8700 - val_loss: 2.3376 - val_accuracy: 0.2300\n",
      "Epoch 420/600\n",
      "800/800 [==============================] - 0s 272us/step - loss: 0.3910 - accuracy: 0.8712 - val_loss: 2.3489 - val_accuracy: 0.2150\n",
      "Epoch 421/600\n",
      "800/800 [==============================] - 0s 272us/step - loss: 0.4230 - accuracy: 0.8562 - val_loss: 2.4244 - val_accuracy: 0.1900\n",
      "Epoch 422/600\n",
      "800/800 [==============================] - 0s 268us/step - loss: 0.3886 - accuracy: 0.8725 - val_loss: 2.3683 - val_accuracy: 0.2550\n",
      "Epoch 423/600\n",
      "800/800 [==============================] - 0s 281us/step - loss: 0.4373 - accuracy: 0.8462 - val_loss: 2.4113 - val_accuracy: 0.2200\n",
      "Epoch 424/600\n",
      "800/800 [==============================] - 0s 267us/step - loss: 0.4251 - accuracy: 0.8637 - val_loss: 2.1721 - val_accuracy: 0.2550\n",
      "Epoch 425/600\n",
      "800/800 [==============================] - 0s 281us/step - loss: 0.4330 - accuracy: 0.8650 - val_loss: 2.3164 - val_accuracy: 0.2450\n",
      "Epoch 426/600\n",
      "800/800 [==============================] - 0s 291us/step - loss: 0.4031 - accuracy: 0.8788 - val_loss: 2.2025 - val_accuracy: 0.2700\n",
      "Epoch 427/600\n",
      "800/800 [==============================] - 0s 294us/step - loss: 0.4369 - accuracy: 0.8600 - val_loss: 2.0943 - val_accuracy: 0.2700\n",
      "Epoch 428/600\n",
      "800/800 [==============================] - 0s 290us/step - loss: 0.3855 - accuracy: 0.8712 - val_loss: 2.3539 - val_accuracy: 0.2050\n",
      "Epoch 429/600\n",
      "800/800 [==============================] - 0s 280us/step - loss: 0.4226 - accuracy: 0.8637 - val_loss: 2.3736 - val_accuracy: 0.2100\n",
      "Epoch 430/600\n",
      "800/800 [==============================] - 0s 295us/step - loss: 0.4536 - accuracy: 0.8512 - val_loss: 2.3557 - val_accuracy: 0.2550\n",
      "Epoch 431/600\n",
      "800/800 [==============================] - 0s 275us/step - loss: 0.4042 - accuracy: 0.8662 - val_loss: 2.4372 - val_accuracy: 0.2050\n",
      "Epoch 432/600\n",
      "800/800 [==============================] - 0s 269us/step - loss: 0.4179 - accuracy: 0.8687 - val_loss: 2.1875 - val_accuracy: 0.2350\n",
      "Epoch 433/600\n",
      "800/800 [==============================] - 0s 295us/step - loss: 0.4440 - accuracy: 0.8637 - val_loss: 2.5302 - val_accuracy: 0.2000\n",
      "Epoch 434/600\n",
      "800/800 [==============================] - 0s 304us/step - loss: 0.4332 - accuracy: 0.8675 - val_loss: 2.2637 - val_accuracy: 0.2400\n",
      "Epoch 435/600\n",
      "800/800 [==============================] - 0s 287us/step - loss: 0.4583 - accuracy: 0.8475 - val_loss: 2.2587 - val_accuracy: 0.2450\n",
      "Epoch 436/600\n",
      "800/800 [==============================] - 0s 275us/step - loss: 0.4084 - accuracy: 0.8662 - val_loss: 2.2653 - val_accuracy: 0.2150\n",
      "Epoch 437/600\n",
      "800/800 [==============================] - 0s 306us/step - loss: 0.3888 - accuracy: 0.8838 - val_loss: 2.2748 - val_accuracy: 0.2350\n",
      "Epoch 438/600\n",
      "800/800 [==============================] - 0s 286us/step - loss: 0.4284 - accuracy: 0.8662 - val_loss: 2.3287 - val_accuracy: 0.2250\n",
      "Epoch 439/600\n",
      "800/800 [==============================] - 0s 274us/step - loss: 0.4205 - accuracy: 0.8525 - val_loss: 2.3610 - val_accuracy: 0.1950\n",
      "Epoch 440/600\n",
      "800/800 [==============================] - 0s 270us/step - loss: 0.4096 - accuracy: 0.8712 - val_loss: 2.3398 - val_accuracy: 0.2000\n",
      "Epoch 441/600\n",
      "800/800 [==============================] - 0s 281us/step - loss: 0.4503 - accuracy: 0.8475 - val_loss: 2.5571 - val_accuracy: 0.1900\n",
      "Epoch 442/600\n",
      "800/800 [==============================] - 0s 282us/step - loss: 0.4568 - accuracy: 0.8363 - val_loss: 2.3365 - val_accuracy: 0.2550\n",
      "Epoch 443/600\n",
      "800/800 [==============================] - 0s 264us/step - loss: 0.4004 - accuracy: 0.8650 - val_loss: 2.2184 - val_accuracy: 0.2800\n",
      "Epoch 444/600\n",
      "800/800 [==============================] - 0s 286us/step - loss: 0.4472 - accuracy: 0.8575 - val_loss: 2.4616 - val_accuracy: 0.2600\n",
      "Epoch 445/600\n",
      "800/800 [==============================] - 0s 265us/step - loss: 0.4146 - accuracy: 0.8512 - val_loss: 2.2800 - val_accuracy: 0.2400\n",
      "Epoch 446/600\n",
      "800/800 [==============================] - 0s 272us/step - loss: 0.4018 - accuracy: 0.8700 - val_loss: 2.5103 - val_accuracy: 0.2150\n",
      "Epoch 447/600\n",
      "800/800 [==============================] - 0s 281us/step - loss: 0.4547 - accuracy: 0.8550 - val_loss: 2.4897 - val_accuracy: 0.2100\n",
      "Epoch 448/600\n",
      "800/800 [==============================] - 0s 288us/step - loss: 0.4178 - accuracy: 0.8587 - val_loss: 2.3211 - val_accuracy: 0.2350\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 449/600\n",
      "800/800 [==============================] - 0s 300us/step - loss: 0.4243 - accuracy: 0.8575 - val_loss: 2.0896 - val_accuracy: 0.2550\n",
      "Epoch 450/600\n",
      "800/800 [==============================] - 0s 256us/step - loss: 0.4633 - accuracy: 0.8600 - val_loss: 2.3332 - val_accuracy: 0.2200\n",
      "Epoch 451/600\n",
      "800/800 [==============================] - 0s 279us/step - loss: 0.4362 - accuracy: 0.8587 - val_loss: 2.2478 - val_accuracy: 0.2650\n",
      "Epoch 452/600\n",
      "800/800 [==============================] - 0s 277us/step - loss: 0.3861 - accuracy: 0.8775 - val_loss: 2.4468 - val_accuracy: 0.1950\n",
      "Epoch 453/600\n",
      "800/800 [==============================] - 0s 281us/step - loss: 0.4049 - accuracy: 0.8725 - val_loss: 2.4038 - val_accuracy: 0.2050\n",
      "Epoch 454/600\n",
      "800/800 [==============================] - 0s 285us/step - loss: 0.4036 - accuracy: 0.8700 - val_loss: 2.3168 - val_accuracy: 0.2150\n",
      "Epoch 455/600\n",
      "800/800 [==============================] - 0s 258us/step - loss: 0.4514 - accuracy: 0.8450 - val_loss: 2.1870 - val_accuracy: 0.2500\n",
      "Epoch 456/600\n",
      "800/800 [==============================] - 0s 284us/step - loss: 0.4249 - accuracy: 0.8537 - val_loss: 2.2208 - val_accuracy: 0.2250\n",
      "Epoch 457/600\n",
      "800/800 [==============================] - 0s 269us/step - loss: 0.4449 - accuracy: 0.8388 - val_loss: 2.1791 - val_accuracy: 0.2800\n",
      "Epoch 458/600\n",
      "800/800 [==============================] - 0s 275us/step - loss: 0.4431 - accuracy: 0.8500 - val_loss: 2.2723 - val_accuracy: 0.2350\n",
      "Epoch 459/600\n",
      "800/800 [==============================] - 0s 266us/step - loss: 0.4195 - accuracy: 0.8512 - val_loss: 2.2502 - val_accuracy: 0.2400\n",
      "Epoch 460/600\n",
      "800/800 [==============================] - 0s 289us/step - loss: 0.4597 - accuracy: 0.8587 - val_loss: 2.5323 - val_accuracy: 0.1850\n",
      "Epoch 461/600\n",
      "800/800 [==============================] - 0s 304us/step - loss: 0.4564 - accuracy: 0.8512 - val_loss: 2.2319 - val_accuracy: 0.2050\n",
      "Epoch 462/600\n",
      "800/800 [==============================] - 0s 289us/step - loss: 0.4817 - accuracy: 0.8313 - val_loss: 2.4488 - val_accuracy: 0.1950\n",
      "Epoch 463/600\n",
      "800/800 [==============================] - 0s 278us/step - loss: 0.4856 - accuracy: 0.8325 - val_loss: 2.3425 - val_accuracy: 0.2150\n",
      "Epoch 464/600\n",
      "800/800 [==============================] - 0s 286us/step - loss: 0.4092 - accuracy: 0.8700 - val_loss: 2.4480 - val_accuracy: 0.1950\n",
      "Epoch 465/600\n",
      "800/800 [==============================] - 0s 284us/step - loss: 0.4290 - accuracy: 0.8687 - val_loss: 2.2379 - val_accuracy: 0.2500\n",
      "Epoch 466/600\n",
      "800/800 [==============================] - 0s 295us/step - loss: 0.4235 - accuracy: 0.8625 - val_loss: 2.3879 - val_accuracy: 0.2000\n",
      "Epoch 467/600\n",
      "800/800 [==============================] - 0s 293us/step - loss: 0.3984 - accuracy: 0.8650 - val_loss: 2.3109 - val_accuracy: 0.2050\n",
      "Epoch 468/600\n",
      "800/800 [==============================] - 0s 265us/step - loss: 0.4004 - accuracy: 0.8650 - val_loss: 2.3590 - val_accuracy: 0.2150\n",
      "Epoch 469/600\n",
      "800/800 [==============================] - 0s 271us/step - loss: 0.4503 - accuracy: 0.8462 - val_loss: 2.4054 - val_accuracy: 0.1950\n",
      "Epoch 470/600\n",
      "800/800 [==============================] - 0s 287us/step - loss: 0.3579 - accuracy: 0.8863 - val_loss: 2.3386 - val_accuracy: 0.2300\n",
      "Epoch 471/600\n",
      "800/800 [==============================] - 0s 254us/step - loss: 0.4690 - accuracy: 0.8438 - val_loss: 2.4228 - val_accuracy: 0.1950\n",
      "Epoch 472/600\n",
      "800/800 [==============================] - 0s 270us/step - loss: 0.4539 - accuracy: 0.8400 - val_loss: 2.1845 - val_accuracy: 0.2250\n",
      "Epoch 473/600\n",
      "800/800 [==============================] - 0s 285us/step - loss: 0.4905 - accuracy: 0.8400 - val_loss: 2.3099 - val_accuracy: 0.2200\n",
      "Epoch 474/600\n",
      "800/800 [==============================] - 0s 267us/step - loss: 0.4679 - accuracy: 0.8425 - val_loss: 2.4208 - val_accuracy: 0.2350\n",
      "Epoch 475/600\n",
      "800/800 [==============================] - 0s 297us/step - loss: 0.4871 - accuracy: 0.8450 - val_loss: 2.3642 - val_accuracy: 0.2150\n",
      "Epoch 476/600\n",
      "800/800 [==============================] - 0s 277us/step - loss: 0.4565 - accuracy: 0.8500 - val_loss: 2.2726 - val_accuracy: 0.2250\n",
      "Epoch 477/600\n",
      "800/800 [==============================] - 0s 302us/step - loss: 0.4895 - accuracy: 0.8225 - val_loss: 2.4528 - val_accuracy: 0.2000\n",
      "Epoch 478/600\n",
      "800/800 [==============================] - 0s 301us/step - loss: 0.4185 - accuracy: 0.8550 - val_loss: 2.1945 - val_accuracy: 0.2800\n",
      "Epoch 479/600\n",
      "800/800 [==============================] - 0s 265us/step - loss: 0.3998 - accuracy: 0.8737 - val_loss: 2.2892 - val_accuracy: 0.2400\n",
      "Epoch 480/600\n",
      "800/800 [==============================] - 0s 273us/step - loss: 0.4124 - accuracy: 0.8700 - val_loss: 2.5509 - val_accuracy: 0.1950\n",
      "Epoch 481/600\n",
      "800/800 [==============================] - 0s 273us/step - loss: 0.4178 - accuracy: 0.8512 - val_loss: 2.4474 - val_accuracy: 0.1950\n",
      "Epoch 482/600\n",
      "800/800 [==============================] - 0s 304us/step - loss: 0.3927 - accuracy: 0.8700 - val_loss: 2.3191 - val_accuracy: 0.2000\n",
      "Epoch 483/600\n",
      "800/800 [==============================] - 0s 283us/step - loss: 0.4417 - accuracy: 0.8475 - val_loss: 2.5842 - val_accuracy: 0.1950\n",
      "Epoch 484/600\n",
      "800/800 [==============================] - 0s 269us/step - loss: 0.4090 - accuracy: 0.8662 - val_loss: 2.5426 - val_accuracy: 0.2000\n",
      "Epoch 485/600\n",
      "800/800 [==============================] - 0s 272us/step - loss: 0.3930 - accuracy: 0.8562 - val_loss: 2.5245 - val_accuracy: 0.1950\n",
      "Epoch 486/600\n",
      "800/800 [==============================] - 0s 301us/step - loss: 0.3687 - accuracy: 0.8737 - val_loss: 2.2497 - val_accuracy: 0.2300\n",
      "Epoch 487/600\n",
      "800/800 [==============================] - 0s 262us/step - loss: 0.4530 - accuracy: 0.8438 - val_loss: 2.2493 - val_accuracy: 0.2400\n",
      "Epoch 488/600\n",
      "800/800 [==============================] - 0s 266us/step - loss: 0.4197 - accuracy: 0.8675 - val_loss: 2.3372 - val_accuracy: 0.2250\n",
      "Epoch 489/600\n",
      "800/800 [==============================] - 0s 266us/step - loss: 0.3865 - accuracy: 0.8750 - val_loss: 2.2020 - val_accuracy: 0.2350\n",
      "Epoch 490/600\n",
      "800/800 [==============================] - 0s 271us/step - loss: 0.4168 - accuracy: 0.8600 - val_loss: 2.3857 - val_accuracy: 0.2000\n",
      "Epoch 491/600\n",
      "800/800 [==============================] - 0s 279us/step - loss: 0.3879 - accuracy: 0.8712 - val_loss: 2.3012 - val_accuracy: 0.2100\n",
      "Epoch 492/600\n",
      "800/800 [==============================] - 0s 298us/step - loss: 0.4399 - accuracy: 0.8600 - val_loss: 2.1977 - val_accuracy: 0.2500\n",
      "Epoch 493/600\n",
      "800/800 [==============================] - 0s 278us/step - loss: 0.4162 - accuracy: 0.8662 - val_loss: 2.3697 - val_accuracy: 0.1950\n",
      "Epoch 494/600\n",
      "800/800 [==============================] - 0s 262us/step - loss: 0.4607 - accuracy: 0.8475 - val_loss: 2.5044 - val_accuracy: 0.1900\n",
      "Epoch 495/600\n",
      "800/800 [==============================] - 0s 294us/step - loss: 0.4451 - accuracy: 0.8425 - val_loss: 2.3898 - val_accuracy: 0.2150\n",
      "Epoch 496/600\n",
      "800/800 [==============================] - 0s 294us/step - loss: 0.4552 - accuracy: 0.8500 - val_loss: 2.3916 - val_accuracy: 0.2050\n",
      "Epoch 497/600\n",
      "800/800 [==============================] - 0s 285us/step - loss: 0.3957 - accuracy: 0.8612 - val_loss: 2.3023 - val_accuracy: 0.2300\n",
      "Epoch 498/600\n",
      "800/800 [==============================] - 0s 274us/step - loss: 0.4182 - accuracy: 0.8562 - val_loss: 2.4584 - val_accuracy: 0.2000\n",
      "Epoch 499/600\n",
      "800/800 [==============================] - 0s 266us/step - loss: 0.4870 - accuracy: 0.8450 - val_loss: 2.3434 - val_accuracy: 0.2000\n",
      "Epoch 500/600\n",
      "800/800 [==============================] - 0s 273us/step - loss: 0.4365 - accuracy: 0.8475 - val_loss: 2.3414 - val_accuracy: 0.2150\n",
      "Epoch 501/600\n",
      "800/800 [==============================] - 0s 282us/step - loss: 0.3821 - accuracy: 0.8838 - val_loss: 2.5138 - val_accuracy: 0.1900\n",
      "Epoch 502/600\n",
      "800/800 [==============================] - 0s 294us/step - loss: 0.4531 - accuracy: 0.8487 - val_loss: 2.2697 - val_accuracy: 0.2150\n",
      "Epoch 503/600\n",
      "800/800 [==============================] - 0s 278us/step - loss: 0.4309 - accuracy: 0.8512 - val_loss: 2.5045 - val_accuracy: 0.1850\n",
      "Epoch 504/600\n",
      "800/800 [==============================] - 0s 272us/step - loss: 0.3855 - accuracy: 0.8600 - val_loss: 2.4786 - val_accuracy: 0.1900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 505/600\n",
      "800/800 [==============================] - 0s 286us/step - loss: 0.4757 - accuracy: 0.8313 - val_loss: 2.2929 - val_accuracy: 0.2000\n",
      "Epoch 506/600\n",
      "800/800 [==============================] - 0s 289us/step - loss: 0.4503 - accuracy: 0.8587 - val_loss: 2.5650 - val_accuracy: 0.1800\n",
      "Epoch 507/600\n",
      "800/800 [==============================] - 0s 299us/step - loss: 0.3952 - accuracy: 0.8562 - val_loss: 2.4413 - val_accuracy: 0.1950\n",
      "Epoch 508/600\n",
      "800/800 [==============================] - 0s 293us/step - loss: 0.4407 - accuracy: 0.8450 - val_loss: 2.4424 - val_accuracy: 0.1900\n",
      "Epoch 509/600\n",
      "800/800 [==============================] - 0s 289us/step - loss: 0.4249 - accuracy: 0.8637 - val_loss: 2.5920 - val_accuracy: 0.1800\n",
      "Epoch 510/600\n",
      "800/800 [==============================] - 0s 285us/step - loss: 0.3945 - accuracy: 0.8788 - val_loss: 2.4810 - val_accuracy: 0.1900\n",
      "Epoch 511/600\n",
      "800/800 [==============================] - 0s 271us/step - loss: 0.4073 - accuracy: 0.8687 - val_loss: 2.3818 - val_accuracy: 0.1950\n",
      "Epoch 512/600\n",
      "800/800 [==============================] - 0s 336us/step - loss: 0.4273 - accuracy: 0.8525 - val_loss: 2.2985 - val_accuracy: 0.2250\n",
      "Epoch 513/600\n",
      "800/800 [==============================] - 0s 277us/step - loss: 0.3898 - accuracy: 0.8700 - val_loss: 2.3390 - val_accuracy: 0.2050\n",
      "Epoch 514/600\n",
      "800/800 [==============================] - 0s 274us/step - loss: 0.4534 - accuracy: 0.8425 - val_loss: 2.5710 - val_accuracy: 0.1700\n",
      "Epoch 515/600\n",
      "800/800 [==============================] - 0s 277us/step - loss: 0.4318 - accuracy: 0.8600 - val_loss: 2.4386 - val_accuracy: 0.1850\n",
      "Epoch 516/600\n",
      "800/800 [==============================] - 0s 299us/step - loss: 0.4408 - accuracy: 0.8512 - val_loss: 2.5156 - val_accuracy: 0.1900\n",
      "Epoch 517/600\n",
      "800/800 [==============================] - 0s 274us/step - loss: 0.4252 - accuracy: 0.8587 - val_loss: 2.6519 - val_accuracy: 0.1800\n",
      "Epoch 518/600\n",
      "800/800 [==============================] - 0s 275us/step - loss: 0.4176 - accuracy: 0.8600 - val_loss: 2.5002 - val_accuracy: 0.1900\n",
      "Epoch 519/600\n",
      "800/800 [==============================] - 0s 285us/step - loss: 0.3851 - accuracy: 0.8725 - val_loss: 2.4125 - val_accuracy: 0.1950\n",
      "Epoch 520/600\n",
      "800/800 [==============================] - 0s 284us/step - loss: 0.4117 - accuracy: 0.8675 - val_loss: 2.4064 - val_accuracy: 0.1900\n",
      "Epoch 521/600\n",
      "800/800 [==============================] - 0s 287us/step - loss: 0.3898 - accuracy: 0.8813 - val_loss: 2.5233 - val_accuracy: 0.1900\n",
      "Epoch 522/600\n",
      "800/800 [==============================] - 0s 302us/step - loss: 0.3758 - accuracy: 0.8650 - val_loss: 2.4120 - val_accuracy: 0.2000\n",
      "Epoch 523/600\n",
      "800/800 [==============================] - 0s 294us/step - loss: 0.3767 - accuracy: 0.8788 - val_loss: 2.4316 - val_accuracy: 0.1950\n",
      "Epoch 524/600\n",
      "800/800 [==============================] - 0s 287us/step - loss: 0.4385 - accuracy: 0.8475 - val_loss: 2.5280 - val_accuracy: 0.1900\n",
      "Epoch 525/600\n",
      "800/800 [==============================] - 0s 305us/step - loss: 0.4462 - accuracy: 0.8575 - val_loss: 2.6308 - val_accuracy: 0.1850\n",
      "Epoch 526/600\n",
      "800/800 [==============================] - 0s 296us/step - loss: 0.4254 - accuracy: 0.8637 - val_loss: 2.3590 - val_accuracy: 0.2100\n",
      "Epoch 527/600\n",
      "800/800 [==============================] - 0s 286us/step - loss: 0.4602 - accuracy: 0.8500 - val_loss: 2.3721 - val_accuracy: 0.1900\n",
      "Epoch 528/600\n",
      "800/800 [==============================] - 0s 307us/step - loss: 0.4079 - accuracy: 0.8650 - val_loss: 2.4676 - val_accuracy: 0.1900\n",
      "Epoch 529/600\n",
      "800/800 [==============================] - 0s 290us/step - loss: 0.4646 - accuracy: 0.8500 - val_loss: 2.4915 - val_accuracy: 0.1900\n",
      "Epoch 530/600\n",
      "800/800 [==============================] - 0s 287us/step - loss: 0.4373 - accuracy: 0.8550 - val_loss: 2.4934 - val_accuracy: 0.2150\n",
      "Epoch 531/600\n",
      "800/800 [==============================] - 0s 298us/step - loss: 0.3999 - accuracy: 0.8625 - val_loss: 2.4946 - val_accuracy: 0.1900\n",
      "Epoch 532/600\n",
      "800/800 [==============================] - 0s 272us/step - loss: 0.3695 - accuracy: 0.8750 - val_loss: 2.3953 - val_accuracy: 0.1900\n",
      "Epoch 533/600\n",
      "800/800 [==============================] - 0s 273us/step - loss: 0.4406 - accuracy: 0.8450 - val_loss: 2.4635 - val_accuracy: 0.1900\n",
      "Epoch 534/600\n",
      "800/800 [==============================] - 0s 301us/step - loss: 0.4567 - accuracy: 0.8450 - val_loss: 2.4298 - val_accuracy: 0.1900\n",
      "Epoch 535/600\n",
      "800/800 [==============================] - 0s 287us/step - loss: 0.4258 - accuracy: 0.8600 - val_loss: 2.4081 - val_accuracy: 0.1900\n",
      "Epoch 536/600\n",
      "800/800 [==============================] - 0s 303us/step - loss: 0.4331 - accuracy: 0.8562 - val_loss: 2.3454 - val_accuracy: 0.1950\n",
      "Epoch 537/600\n",
      "800/800 [==============================] - 0s 317us/step - loss: 0.4380 - accuracy: 0.8425 - val_loss: 2.3941 - val_accuracy: 0.2050\n",
      "Epoch 538/600\n",
      "800/800 [==============================] - 0s 308us/step - loss: 0.3963 - accuracy: 0.8637 - val_loss: 2.2631 - val_accuracy: 0.1950\n",
      "Epoch 539/600\n",
      "800/800 [==============================] - 0s 270us/step - loss: 0.4139 - accuracy: 0.8587 - val_loss: 2.3816 - val_accuracy: 0.2050\n",
      "Epoch 540/600\n",
      "800/800 [==============================] - 0s 285us/step - loss: 0.4174 - accuracy: 0.8687 - val_loss: 2.5104 - val_accuracy: 0.1800\n",
      "Epoch 541/600\n",
      "800/800 [==============================] - 0s 294us/step - loss: 0.4304 - accuracy: 0.8512 - val_loss: 2.6232 - val_accuracy: 0.1850\n",
      "Epoch 542/600\n",
      "800/800 [==============================] - 0s 295us/step - loss: 0.4076 - accuracy: 0.8625 - val_loss: 2.2860 - val_accuracy: 0.2200\n",
      "Epoch 543/600\n",
      "800/800 [==============================] - 0s 283us/step - loss: 0.4522 - accuracy: 0.8550 - val_loss: 2.3101 - val_accuracy: 0.2000\n",
      "Epoch 544/600\n",
      "800/800 [==============================] - 0s 304us/step - loss: 0.4279 - accuracy: 0.8537 - val_loss: 2.5002 - val_accuracy: 0.2000\n",
      "Epoch 545/600\n",
      "800/800 [==============================] - 0s 279us/step - loss: 0.4378 - accuracy: 0.8438 - val_loss: 2.2997 - val_accuracy: 0.2150\n",
      "Epoch 546/600\n",
      "800/800 [==============================] - 0s 275us/step - loss: 0.4240 - accuracy: 0.8525 - val_loss: 2.4965 - val_accuracy: 0.1850\n",
      "Epoch 547/600\n",
      "800/800 [==============================] - 0s 253us/step - loss: 0.4366 - accuracy: 0.8587 - val_loss: 2.3614 - val_accuracy: 0.2350\n",
      "Epoch 548/600\n",
      "800/800 [==============================] - 0s 295us/step - loss: 0.3765 - accuracy: 0.8700 - val_loss: 2.3250 - val_accuracy: 0.2000\n",
      "Epoch 549/600\n",
      "800/800 [==============================] - 0s 297us/step - loss: 0.3937 - accuracy: 0.8700 - val_loss: 2.3113 - val_accuracy: 0.2300\n",
      "Epoch 550/600\n",
      "800/800 [==============================] - 0s 269us/step - loss: 0.3875 - accuracy: 0.8788 - val_loss: 2.2632 - val_accuracy: 0.2500\n",
      "Epoch 551/600\n",
      "800/800 [==============================] - 0s 262us/step - loss: 0.4092 - accuracy: 0.8575 - val_loss: 2.5052 - val_accuracy: 0.1800\n",
      "Epoch 552/600\n",
      "800/800 [==============================] - 0s 280us/step - loss: 0.4044 - accuracy: 0.8637 - val_loss: 2.2859 - val_accuracy: 0.2000\n",
      "Epoch 553/600\n",
      "800/800 [==============================] - 0s 292us/step - loss: 0.4305 - accuracy: 0.8500 - val_loss: 2.3927 - val_accuracy: 0.1950\n",
      "Epoch 554/600\n",
      "800/800 [==============================] - 0s 291us/step - loss: 0.4390 - accuracy: 0.8525 - val_loss: 2.6848 - val_accuracy: 0.1850\n",
      "Epoch 555/600\n",
      "800/800 [==============================] - 0s 293us/step - loss: 0.4557 - accuracy: 0.8413 - val_loss: 2.3986 - val_accuracy: 0.2050\n",
      "Epoch 556/600\n",
      "800/800 [==============================] - 0s 291us/step - loss: 0.4388 - accuracy: 0.8512 - val_loss: 2.5143 - val_accuracy: 0.1900\n",
      "Epoch 557/600\n",
      "800/800 [==============================] - 0s 263us/step - loss: 0.3724 - accuracy: 0.8725 - val_loss: 2.4534 - val_accuracy: 0.1900\n",
      "Epoch 558/600\n",
      "800/800 [==============================] - 0s 281us/step - loss: 0.3985 - accuracy: 0.8550 - val_loss: 2.5050 - val_accuracy: 0.1900\n",
      "Epoch 559/600\n",
      "800/800 [==============================] - 0s 270us/step - loss: 0.4726 - accuracy: 0.8450 - val_loss: 2.4078 - val_accuracy: 0.1900\n",
      "Epoch 560/600\n",
      "800/800 [==============================] - 0s 294us/step - loss: 0.3942 - accuracy: 0.8637 - val_loss: 2.4559 - val_accuracy: 0.1900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 561/600\n",
      "800/800 [==============================] - 0s 299us/step - loss: 0.4057 - accuracy: 0.8662 - val_loss: 2.4752 - val_accuracy: 0.1950\n",
      "Epoch 562/600\n",
      "800/800 [==============================] - 0s 290us/step - loss: 0.3730 - accuracy: 0.8800 - val_loss: 2.3217 - val_accuracy: 0.2300\n",
      "Epoch 563/600\n",
      "800/800 [==============================] - 0s 308us/step - loss: 0.4247 - accuracy: 0.8625 - val_loss: 2.3718 - val_accuracy: 0.1950\n",
      "Epoch 564/600\n",
      "800/800 [==============================] - 0s 276us/step - loss: 0.4274 - accuracy: 0.8575 - val_loss: 2.4200 - val_accuracy: 0.1950\n",
      "Epoch 565/600\n",
      "800/800 [==============================] - 0s 278us/step - loss: 0.4061 - accuracy: 0.8675 - val_loss: 2.6386 - val_accuracy: 0.1900\n",
      "Epoch 566/600\n",
      "800/800 [==============================] - 0s 289us/step - loss: 0.4554 - accuracy: 0.8450 - val_loss: 2.2181 - val_accuracy: 0.2150\n",
      "Epoch 567/600\n",
      "800/800 [==============================] - 0s 273us/step - loss: 0.4185 - accuracy: 0.8512 - val_loss: 2.5987 - val_accuracy: 0.1900\n",
      "Epoch 568/600\n",
      "800/800 [==============================] - 0s 276us/step - loss: 0.3892 - accuracy: 0.8650 - val_loss: 2.4499 - val_accuracy: 0.1950\n",
      "Epoch 569/600\n",
      "800/800 [==============================] - 0s 288us/step - loss: 0.3963 - accuracy: 0.8763 - val_loss: 2.3827 - val_accuracy: 0.2300\n",
      "Epoch 570/600\n",
      "800/800 [==============================] - 0s 279us/step - loss: 0.4157 - accuracy: 0.8662 - val_loss: 2.4949 - val_accuracy: 0.1900\n",
      "Epoch 571/600\n",
      "800/800 [==============================] - 0s 266us/step - loss: 0.4402 - accuracy: 0.8500 - val_loss: 2.5265 - val_accuracy: 0.1950\n",
      "Epoch 572/600\n",
      "800/800 [==============================] - 0s 309us/step - loss: 0.4352 - accuracy: 0.8562 - val_loss: 2.3221 - val_accuracy: 0.2050\n",
      "Epoch 573/600\n",
      "800/800 [==============================] - 0s 314us/step - loss: 0.3768 - accuracy: 0.8838 - val_loss: 2.4337 - val_accuracy: 0.1900\n",
      "Epoch 574/600\n",
      "800/800 [==============================] - 0s 308us/step - loss: 0.3820 - accuracy: 0.8775 - val_loss: 2.3414 - val_accuracy: 0.2000\n",
      "Epoch 575/600\n",
      "800/800 [==============================] - 0s 288us/step - loss: 0.4040 - accuracy: 0.8662 - val_loss: 2.2059 - val_accuracy: 0.2500\n",
      "Epoch 576/600\n",
      "800/800 [==============================] - 0s 271us/step - loss: 0.4002 - accuracy: 0.8725 - val_loss: 2.3963 - val_accuracy: 0.2000\n",
      "Epoch 577/600\n",
      "800/800 [==============================] - 0s 299us/step - loss: 0.3801 - accuracy: 0.8750 - val_loss: 2.6067 - val_accuracy: 0.1900\n",
      "Epoch 578/600\n",
      "800/800 [==============================] - 0s 305us/step - loss: 0.3639 - accuracy: 0.8863 - val_loss: 2.3745 - val_accuracy: 0.1950\n",
      "Epoch 579/600\n",
      "800/800 [==============================] - 0s 283us/step - loss: 0.4328 - accuracy: 0.8400 - val_loss: 2.0893 - val_accuracy: 0.2100\n",
      "Epoch 580/600\n",
      "800/800 [==============================] - 0s 303us/step - loss: 0.3961 - accuracy: 0.8650 - val_loss: 2.4076 - val_accuracy: 0.2250\n",
      "Epoch 581/600\n",
      "800/800 [==============================] - 0s 264us/step - loss: 0.4087 - accuracy: 0.8675 - val_loss: 2.5709 - val_accuracy: 0.1850\n",
      "Epoch 582/600\n",
      " 32/800 [>.............................] - ETA: 0s - loss: 0.4965 - accuracy: 0.7812"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-1348a69cbcf0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m600\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mreduce_lr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# plt.imshow(x[0].reshape(28,28))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"digits_shitty_model.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3475\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3476\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3477\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3478\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "history = classifier.fit(x, y, epochs=600, batch_size=32, callbacks=[reduce_lr], validation_split=0.2)\n",
    "\n",
    "# plt.imshow(x[0].reshape(28,28))\n",
    "classifier.save(\"digits_shitty_model.h5\")\n",
    "classifier.evaluate(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4000, 4)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f08bcc45f10>]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.__dict__['history']['loss'])\n",
    "plt.plot(history.__dict__['history']['accuracy'])\n",
    "# plt.plot(history.__dict__['history']['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Trains a GDenseNet-40-12 model on the CIFAR-10 Dataset.\n",
    "'''\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "from keras.datasets import cifar10\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils import np_utils\n",
    "from keras_gcnn.applications.densenetnew import GDenseNet\n",
    "\n",
    "batch_size = 16\n",
    "nb_classes = 5\n",
    "epochs = 100\n",
    "\n",
    "img_rows, img_cols = 28, 28\n",
    "img_channels = 1\n",
    "\n",
    "# Parameters for the DenseNet model builder\n",
    "img_dim = (img_channels, img_rows, img_cols) if K.image_data_format() == 'channels_first' else (\n",
    "    img_rows, img_cols, img_channels)\n",
    "depth = 10\n",
    "nb_dense_block = 3\n",
    "growth_rate = 3  # number of z2 maps equals growth_rate * group_size, so keep this small.\n",
    "nb_filter = 16\n",
    "dropout_rate = 0.4  # 0.0 for data augmentation\n",
    "conv_group = 'C4'  # C4 includes 90 degree rotations, D4 additionally includes reflections in x and y axis.\n",
    "use_gcnn = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb_layers computed: [2, 2, 2] 2\n",
      "Model created\n",
      "Model: \"densenet\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 28, 28, 1)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "initial_Gconv2D (GConv2D)       (None, 28, 28, 64)   144         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_0_0_bn (GBatchNorm)       (None, 28, 28, 64)   64          initial_Gconv2D[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 28, 28, 64)   0           dense_0_0_bn[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_0_0_Gconv2D (GConv2D)     (None, 28, 28, 12)   1728        activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 28, 28, 12)   0           dense_0_0_Gconv2D[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_43 (Concatenate)    (None, 28, 28, 76)   0           initial_Gconv2D[0][0]            \n",
      "                                                                 dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_0_1_bn (GBatchNorm)       (None, 28, 28, 76)   76          concatenate_43[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 28, 28, 76)   0           dense_0_1_bn[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_0_1_Gconv2D (GConv2D)     (None, 28, 28, 12)   2052        activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 28, 28, 12)   0           dense_0_1_Gconv2D[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_44 (Concatenate)    (None, 28, 28, 88)   0           concatenate_43[0][0]             \n",
      "                                                                 dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tr_0_bn (GBatchNorm)            (None, 28, 28, 88)   88          concatenate_44[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 28, 28, 88)   0           tr_0_bn[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tr_0_Gconv2D (GConv2D)          (None, 28, 28, 88)   1936        activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_5 (AveragePoo (None, 14, 14, 88)   0           tr_0_Gconv2D[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_1_0_bn (GBatchNorm)       (None, 14, 14, 88)   88          average_pooling2d_5[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 14, 14, 88)   0           dense_1_0_bn[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_1_0_Gconv2D (GConv2D)     (None, 14, 14, 12)   2376        activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 14, 14, 12)   0           dense_1_0_Gconv2D[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_45 (Concatenate)    (None, 14, 14, 100)  0           average_pooling2d_5[0][0]        \n",
      "                                                                 dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1_1_bn (GBatchNorm)       (None, 14, 14, 100)  100         concatenate_45[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 14, 14, 100)  0           dense_1_1_bn[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_1_1_Gconv2D (GConv2D)     (None, 14, 14, 12)   2700        activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 14, 14, 12)   0           dense_1_1_Gconv2D[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_46 (Concatenate)    (None, 14, 14, 112)  0           concatenate_45[0][0]             \n",
      "                                                                 dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tr_1_bn (GBatchNorm)            (None, 14, 14, 112)  112         concatenate_46[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 14, 14, 112)  0           tr_1_bn[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tr_1_Gconv2D (GConv2D)          (None, 14, 14, 112)  3136        activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_6 (AveragePoo (None, 7, 7, 112)    0           tr_1_Gconv2D[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_2_0_bn (GBatchNorm)       (None, 7, 7, 112)    112         average_pooling2d_6[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 7, 7, 112)    0           dense_2_0_bn[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_2_0_Gconv2D (GConv2D)     (None, 7, 7, 12)     3024        activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 7, 7, 12)     0           dense_2_0_Gconv2D[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_47 (Concatenate)    (None, 7, 7, 124)    0           average_pooling2d_6[0][0]        \n",
      "                                                                 dropout_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_2_1_bn (GBatchNorm)       (None, 7, 7, 124)    124         concatenate_47[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 7, 7, 124)    0           dense_2_1_bn[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_2_1_Gconv2D (GConv2D)     (None, 7, 7, 12)     3348        activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)            (None, 7, 7, 12)     0           dense_2_1_Gconv2D[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_48 (Concatenate)    (None, 7, 7, 136)    0           concatenate_47[0][0]             \n",
      "                                                                 dropout_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "final_bn (GBatchNorm)           (None, 7, 7, 136)    136         concatenate_48[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 7, 7, 136)    0           final_bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "group_pool_3 (GroupPool)        (None, 7, 7, 34)     0           activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_3 (Glo (None, 34)           0           group_pool_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 5)            175         global_average_pooling2d_3[0][0] \n",
      "==================================================================================================\n",
      "Total params: 21,519\n",
      "Trainable params: 21,069\n",
      "Non-trainable params: 450\n",
      "__________________________________________________________________________________________________\n",
      "Finished compiling\n"
     ]
    }
   ],
   "source": [
    "# Create the model (without loading weights)\n",
    "model = GDenseNet(mc_dropout=False, padding='same', nb_dense_block=nb_dense_block, growth_rate=growth_rate,\n",
    "                  nb_filter=nb_filter, dropout_rate=dropout_rate, weights=None, input_shape=img_dim, depth=depth,\n",
    "                  use_gcnn=use_gcnn, conv_group=conv_group, classes=nb_classes)\n",
    "print('Model created')\n",
    "\n",
    "model.summary()\n",
    "\n",
    "optimizer = Adam(lr=1e-3)  # Using Adam instead of SGD to speed up training\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['acc'])\n",
    "print('Finished compiling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 800 samples, validate on 200 samples\n",
      "Epoch 1/100\n",
      "800/800 [==============================] - 1s 748us/step - loss: 0.1837 - acc: 0.9925 - val_loss: 0.8429 - val_acc: 0.6450\n",
      "Epoch 2/100\n",
      "800/800 [==============================] - 1s 755us/step - loss: 0.1825 - acc: 0.9900 - val_loss: 0.8315 - val_acc: 0.6850\n",
      "Epoch 3/100\n",
      "800/800 [==============================] - 1s 732us/step - loss: 0.1957 - acc: 0.9900 - val_loss: 0.8616 - val_acc: 0.6800\n",
      "Epoch 4/100\n",
      "800/800 [==============================] - 1s 738us/step - loss: 0.1811 - acc: 0.9987 - val_loss: 0.6525 - val_acc: 0.7550\n",
      "Epoch 5/100\n",
      "800/800 [==============================] - 1s 728us/step - loss: 0.1803 - acc: 0.9925 - val_loss: 0.6036 - val_acc: 0.7800\n",
      "Epoch 6/100\n",
      "800/800 [==============================] - 1s 752us/step - loss: 0.1809 - acc: 0.9937 - val_loss: 0.5529 - val_acc: 0.7800\n",
      "Epoch 7/100\n",
      "800/800 [==============================] - 1s 746us/step - loss: 0.1819 - acc: 0.9925 - val_loss: 0.5748 - val_acc: 0.8050\n",
      "Epoch 8/100\n",
      "800/800 [==============================] - 1s 749us/step - loss: 0.1781 - acc: 0.9887 - val_loss: 0.5799 - val_acc: 0.8000\n",
      "Epoch 9/100\n",
      "800/800 [==============================] - 1s 761us/step - loss: 0.1808 - acc: 0.9937 - val_loss: 0.6720 - val_acc: 0.7500\n",
      "Epoch 10/100\n",
      "800/800 [==============================] - 1s 760us/step - loss: 0.1720 - acc: 0.9987 - val_loss: 0.4911 - val_acc: 0.8200\n",
      "Epoch 11/100\n",
      "800/800 [==============================] - 1s 761us/step - loss: 0.1783 - acc: 0.9862 - val_loss: 0.6756 - val_acc: 0.7250\n",
      "Epoch 12/100\n",
      "800/800 [==============================] - 1s 760us/step - loss: 0.1770 - acc: 0.9975 - val_loss: 0.8600 - val_acc: 0.6750\n",
      "Epoch 13/100\n",
      "800/800 [==============================] - 1s 718us/step - loss: 0.1721 - acc: 0.9937 - val_loss: 0.7958 - val_acc: 0.6950\n",
      "Epoch 14/100\n",
      "800/800 [==============================] - 1s 768us/step - loss: 0.1843 - acc: 0.9875 - val_loss: 0.8558 - val_acc: 0.6500\n",
      "Epoch 15/100\n",
      "800/800 [==============================] - 1s 750us/step - loss: 0.1705 - acc: 0.9900 - val_loss: 0.8229 - val_acc: 0.6650\n",
      "Epoch 16/100\n",
      "800/800 [==============================] - 1s 752us/step - loss: 0.1764 - acc: 0.9975 - val_loss: 0.7060 - val_acc: 0.7250\n",
      "Epoch 17/100\n",
      "800/800 [==============================] - 1s 734us/step - loss: 0.1772 - acc: 0.9925 - val_loss: 0.6021 - val_acc: 0.7750\n",
      "Epoch 18/100\n",
      "800/800 [==============================] - 1s 752us/step - loss: 0.1732 - acc: 0.9962 - val_loss: 0.7248 - val_acc: 0.7200\n",
      "Epoch 19/100\n",
      "800/800 [==============================] - 1s 728us/step - loss: 0.1780 - acc: 0.9900 - val_loss: 0.8101 - val_acc: 0.6800\n",
      "Epoch 20/100\n",
      "800/800 [==============================] - 1s 738us/step - loss: 0.1702 - acc: 0.9925 - val_loss: 1.0205 - val_acc: 0.6200\n",
      "Epoch 21/100\n",
      "800/800 [==============================] - 1s 728us/step - loss: 0.1616 - acc: 0.9962 - val_loss: 1.0557 - val_acc: 0.6300\n",
      "Epoch 22/100\n",
      "800/800 [==============================] - 1s 736us/step - loss: 0.1707 - acc: 0.9937 - val_loss: 0.9391 - val_acc: 0.6550\n",
      "Epoch 23/100\n",
      "800/800 [==============================] - 1s 732us/step - loss: 0.1762 - acc: 0.9975 - val_loss: 0.7985 - val_acc: 0.6850\n",
      "Epoch 24/100\n",
      "800/800 [==============================] - 1s 719us/step - loss: 0.1717 - acc: 0.9925 - val_loss: 0.7291 - val_acc: 0.7150\n",
      "Epoch 25/100\n",
      "800/800 [==============================] - 1s 737us/step - loss: 0.1685 - acc: 0.9962 - val_loss: 0.6642 - val_acc: 0.7600\n",
      "Epoch 26/100\n",
      "800/800 [==============================] - 1s 732us/step - loss: 0.1681 - acc: 0.9912 - val_loss: 0.7271 - val_acc: 0.7150\n",
      "Epoch 27/100\n",
      "800/800 [==============================] - 1s 748us/step - loss: 0.1667 - acc: 0.9937 - val_loss: 0.8252 - val_acc: 0.6800\n",
      "Epoch 28/100\n",
      "800/800 [==============================] - 1s 737us/step - loss: 0.1683 - acc: 0.9937 - val_loss: 0.8800 - val_acc: 0.6700\n",
      "Epoch 29/100\n",
      "800/800 [==============================] - 1s 719us/step - loss: 0.1807 - acc: 0.9887 - val_loss: 0.8819 - val_acc: 0.6700\n",
      "Epoch 30/100\n",
      "800/800 [==============================] - 1s 773us/step - loss: 0.1665 - acc: 0.9950 - val_loss: 0.9143 - val_acc: 0.6600\n",
      "Epoch 31/100\n",
      "800/800 [==============================] - 1s 728us/step - loss: 0.1722 - acc: 0.9900 - val_loss: 0.9122 - val_acc: 0.6600\n",
      "Epoch 32/100\n",
      "800/800 [==============================] - 1s 751us/step - loss: 0.1707 - acc: 0.9937 - val_loss: 0.8799 - val_acc: 0.6700\n",
      "Epoch 33/100\n",
      "800/800 [==============================] - 1s 754us/step - loss: 0.1635 - acc: 0.9950 - val_loss: 0.8803 - val_acc: 0.6700\n",
      "Epoch 34/100\n",
      "800/800 [==============================] - 1s 710us/step - loss: 0.1703 - acc: 0.9912 - val_loss: 0.8681 - val_acc: 0.6700\n",
      "Epoch 35/100\n",
      "800/800 [==============================] - 1s 750us/step - loss: 0.1783 - acc: 0.9887 - val_loss: 0.8542 - val_acc: 0.6750\n",
      "Epoch 36/100\n",
      "800/800 [==============================] - 1s 739us/step - loss: 0.1682 - acc: 0.9950 - val_loss: 0.8485 - val_acc: 0.6750\n",
      "Epoch 37/100\n",
      "800/800 [==============================] - 1s 748us/step - loss: 0.1637 - acc: 0.9925 - val_loss: 0.8317 - val_acc: 0.6800\n",
      "Epoch 38/100\n",
      "800/800 [==============================] - 1s 736us/step - loss: 0.1789 - acc: 0.9887 - val_loss: 0.8025 - val_acc: 0.6950\n",
      "Epoch 39/100\n",
      "800/800 [==============================] - 1s 741us/step - loss: 0.1728 - acc: 0.9925 - val_loss: 0.7804 - val_acc: 0.6950\n",
      "Epoch 40/100\n",
      "800/800 [==============================] - 1s 727us/step - loss: 0.1756 - acc: 0.9912 - val_loss: 0.7732 - val_acc: 0.6950\n",
      "Epoch 41/100\n",
      "800/800 [==============================] - 1s 700us/step - loss: 0.1598 - acc: 0.9962 - val_loss: 0.7873 - val_acc: 0.6950\n",
      "Epoch 42/100\n",
      "800/800 [==============================] - 1s 781us/step - loss: 0.1665 - acc: 0.9962 - val_loss: 0.7866 - val_acc: 0.6950\n",
      "Epoch 43/100\n",
      "800/800 [==============================] - 1s 730us/step - loss: 0.1615 - acc: 0.9937 - val_loss: 0.7849 - val_acc: 0.6950\n",
      "Epoch 44/100\n",
      "800/800 [==============================] - 1s 731us/step - loss: 0.1756 - acc: 0.9912 - val_loss: 0.7936 - val_acc: 0.6950\n",
      "Epoch 45/100\n",
      "800/800 [==============================] - 1s 735us/step - loss: 0.1735 - acc: 0.9875 - val_loss: 0.7971 - val_acc: 0.6950\n",
      "Epoch 46/100\n",
      "800/800 [==============================] - 1s 743us/step - loss: 0.1636 - acc: 0.9950 - val_loss: 0.8056 - val_acc: 0.6950\n",
      "Epoch 47/100\n",
      "800/800 [==============================] - 1s 725us/step - loss: 0.1688 - acc: 0.9950 - val_loss: 0.8149 - val_acc: 0.6850\n",
      "Epoch 48/100\n",
      "800/800 [==============================] - 1s 715us/step - loss: 0.1626 - acc: 0.9962 - val_loss: 0.8129 - val_acc: 0.6900\n",
      "Epoch 49/100\n",
      "800/800 [==============================] - 1s 732us/step - loss: 0.1700 - acc: 0.9900 - val_loss: 0.8130 - val_acc: 0.6900\n",
      "Epoch 50/100\n",
      "800/800 [==============================] - 1s 740us/step - loss: 0.1771 - acc: 0.9887 - val_loss: 0.8086 - val_acc: 0.6900\n",
      "Epoch 51/100\n",
      "800/800 [==============================] - 1s 714us/step - loss: 0.1701 - acc: 0.9937 - val_loss: 0.8159 - val_acc: 0.6850\n",
      "Epoch 52/100\n",
      "800/800 [==============================] - 1s 746us/step - loss: 0.1693 - acc: 0.9912 - val_loss: 0.8167 - val_acc: 0.6850\n",
      "Epoch 53/100\n",
      "800/800 [==============================] - 1s 723us/step - loss: 0.1733 - acc: 0.9887 - val_loss: 0.8172 - val_acc: 0.6850\n",
      "Epoch 54/100\n",
      "800/800 [==============================] - 1s 724us/step - loss: 0.1664 - acc: 0.9962 - val_loss: 0.8229 - val_acc: 0.6850\n",
      "Epoch 55/100\n",
      "800/800 [==============================] - 1s 742us/step - loss: 0.1630 - acc: 0.9962 - val_loss: 0.8258 - val_acc: 0.6800\n",
      "Epoch 56/100\n",
      "800/800 [==============================] - 1s 715us/step - loss: 0.1719 - acc: 0.9912 - val_loss: 0.8229 - val_acc: 0.6800\n",
      "Epoch 57/100\n",
      "800/800 [==============================] - 1s 727us/step - loss: 0.1621 - acc: 0.9950 - val_loss: 0.8289 - val_acc: 0.6750\n",
      "Epoch 58/100\n",
      "800/800 [==============================] - 1s 733us/step - loss: 0.1692 - acc: 0.9925 - val_loss: 0.8274 - val_acc: 0.6750\n",
      "Epoch 59/100\n",
      "800/800 [==============================] - 1s 775us/step - loss: 0.1576 - acc: 0.9962 - val_loss: 0.8273 - val_acc: 0.6750\n",
      "Epoch 60/100\n",
      "800/800 [==============================] - 1s 726us/step - loss: 0.1638 - acc: 0.9925 - val_loss: 0.8285 - val_acc: 0.6800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/100\n",
      "800/800 [==============================] - 1s 743us/step - loss: 0.1604 - acc: 0.9950 - val_loss: 0.8318 - val_acc: 0.6800\n",
      "Epoch 62/100\n",
      "800/800 [==============================] - 1s 745us/step - loss: 0.1767 - acc: 0.9912 - val_loss: 0.8354 - val_acc: 0.6800\n",
      "Epoch 63/100\n",
      "800/800 [==============================] - 1s 751us/step - loss: 0.1686 - acc: 0.9950 - val_loss: 0.8339 - val_acc: 0.6750\n",
      "Epoch 64/100\n",
      "800/800 [==============================] - 1s 763us/step - loss: 0.1679 - acc: 0.9925 - val_loss: 0.8391 - val_acc: 0.6750\n",
      "Epoch 65/100\n",
      "800/800 [==============================] - 1s 745us/step - loss: 0.1629 - acc: 0.9975 - val_loss: 0.8366 - val_acc: 0.6750\n",
      "Epoch 66/100\n",
      "800/800 [==============================] - 1s 744us/step - loss: 0.1655 - acc: 0.9950 - val_loss: 0.8316 - val_acc: 0.6800\n",
      "Epoch 67/100\n",
      "800/800 [==============================] - 1s 755us/step - loss: 0.1736 - acc: 0.9975 - val_loss: 0.8251 - val_acc: 0.6850\n",
      "Epoch 68/100\n",
      "800/800 [==============================] - 1s 722us/step - loss: 0.1763 - acc: 0.9937 - val_loss: 0.8275 - val_acc: 0.6800\n",
      "Epoch 69/100\n",
      "800/800 [==============================] - 1s 751us/step - loss: 0.1773 - acc: 0.9900 - val_loss: 0.8251 - val_acc: 0.6800\n",
      "Epoch 70/100\n",
      "800/800 [==============================] - 1s 750us/step - loss: 0.1706 - acc: 0.9887 - val_loss: 0.8197 - val_acc: 0.6850\n",
      "Epoch 71/100\n",
      "800/800 [==============================] - 1s 746us/step - loss: 0.1751 - acc: 0.9925 - val_loss: 0.8228 - val_acc: 0.6850\n",
      "Epoch 72/100\n",
      "800/800 [==============================] - 1s 728us/step - loss: 0.1671 - acc: 0.9937 - val_loss: 0.8279 - val_acc: 0.6800\n",
      "Epoch 73/100\n",
      "800/800 [==============================] - 1s 758us/step - loss: 0.1656 - acc: 0.9925 - val_loss: 0.8327 - val_acc: 0.6800\n",
      "Epoch 74/100\n",
      "800/800 [==============================] - 1s 747us/step - loss: 0.1721 - acc: 0.9925 - val_loss: 0.8310 - val_acc: 0.6800\n",
      "Epoch 75/100\n",
      "800/800 [==============================] - 1s 752us/step - loss: 0.1623 - acc: 0.9950 - val_loss: 0.8277 - val_acc: 0.6800\n",
      "Epoch 76/100\n",
      "800/800 [==============================] - 1s 735us/step - loss: 0.1636 - acc: 0.9912 - val_loss: 0.8294 - val_acc: 0.6800\n",
      "Epoch 77/100\n",
      "800/800 [==============================] - 1s 748us/step - loss: 0.1660 - acc: 0.9937 - val_loss: 0.8239 - val_acc: 0.6800\n",
      "Epoch 78/100\n",
      "800/800 [==============================] - 1s 758us/step - loss: 0.1690 - acc: 0.9925 - val_loss: 0.8229 - val_acc: 0.6850\n",
      "Epoch 79/100\n",
      "800/800 [==============================] - 1s 718us/step - loss: 0.1596 - acc: 0.9987 - val_loss: 0.8336 - val_acc: 0.6800\n",
      "Epoch 80/100\n",
      "800/800 [==============================] - 1s 710us/step - loss: 0.1627 - acc: 0.9937 - val_loss: 0.8357 - val_acc: 0.6800\n",
      "Epoch 81/100\n",
      "800/800 [==============================] - 1s 736us/step - loss: 0.1697 - acc: 0.9950 - val_loss: 0.8367 - val_acc: 0.6800\n",
      "Epoch 82/100\n",
      "800/800 [==============================] - 1s 749us/step - loss: 0.1629 - acc: 0.9962 - val_loss: 0.8244 - val_acc: 0.6800\n",
      "Epoch 83/100\n",
      "800/800 [==============================] - 1s 704us/step - loss: 0.1840 - acc: 0.9875 - val_loss: 0.8201 - val_acc: 0.6850\n",
      "Epoch 84/100\n",
      "800/800 [==============================] - 1s 731us/step - loss: 0.1800 - acc: 0.9925 - val_loss: 0.8188 - val_acc: 0.6850\n",
      "Epoch 85/100\n",
      "800/800 [==============================] - 1s 720us/step - loss: 0.1659 - acc: 0.9925 - val_loss: 0.8191 - val_acc: 0.6850\n",
      "Epoch 86/100\n",
      "800/800 [==============================] - 1s 748us/step - loss: 0.1592 - acc: 0.9987 - val_loss: 0.8199 - val_acc: 0.6850\n",
      "Epoch 87/100\n",
      "800/800 [==============================] - 1s 774us/step - loss: 0.1711 - acc: 0.9937 - val_loss: 0.8244 - val_acc: 0.6800\n",
      "Epoch 88/100\n",
      "800/800 [==============================] - 1s 716us/step - loss: 0.1652 - acc: 0.9912 - val_loss: 0.8307 - val_acc: 0.6800\n",
      "Epoch 89/100\n",
      "800/800 [==============================] - 1s 729us/step - loss: 0.1719 - acc: 0.9925 - val_loss: 0.8363 - val_acc: 0.6800\n",
      "Epoch 90/100\n",
      "800/800 [==============================] - 1s 764us/step - loss: 0.1712 - acc: 0.9925 - val_loss: 0.8339 - val_acc: 0.6800\n",
      "Epoch 91/100\n",
      "800/800 [==============================] - 1s 710us/step - loss: 0.1596 - acc: 0.9975 - val_loss: 0.8281 - val_acc: 0.6800\n",
      "Epoch 92/100\n",
      "800/800 [==============================] - 1s 755us/step - loss: 0.1687 - acc: 0.9962 - val_loss: 0.8277 - val_acc: 0.6800\n",
      "Epoch 93/100\n",
      "800/800 [==============================] - 1s 720us/step - loss: 0.1776 - acc: 0.9925 - val_loss: 0.8328 - val_acc: 0.6800\n",
      "Epoch 94/100\n",
      "800/800 [==============================] - 1s 718us/step - loss: 0.1644 - acc: 0.9937 - val_loss: 0.8318 - val_acc: 0.6800\n",
      "Epoch 95/100\n",
      "800/800 [==============================] - 1s 747us/step - loss: 0.1678 - acc: 0.9962 - val_loss: 0.8277 - val_acc: 0.6800\n",
      "Epoch 96/100\n",
      "800/800 [==============================] - 1s 734us/step - loss: 0.1727 - acc: 0.9950 - val_loss: 0.8318 - val_acc: 0.6800\n",
      "Epoch 97/100\n",
      "800/800 [==============================] - 1s 752us/step - loss: 0.1708 - acc: 0.9950 - val_loss: 0.8302 - val_acc: 0.6800\n",
      "Epoch 98/100\n",
      "800/800 [==============================] - 1s 742us/step - loss: 0.1683 - acc: 0.9887 - val_loss: 0.8266 - val_acc: 0.6800\n",
      "Epoch 99/100\n",
      "800/800 [==============================] - 1s 733us/step - loss: 0.1648 - acc: 0.9950 - val_loss: 0.8219 - val_acc: 0.6800\n",
      "Epoch 100/100\n",
      "800/800 [==============================] - 1s 776us/step - loss: 0.1671 - acc: 0.9950 - val_loss: 0.8235 - val_acc: 0.6800\n"
     ]
    }
   ],
   "source": [
    "lr_reducer = ReduceLROnPlateau(monitor='val_loss', factor=np.sqrt(0.1),\n",
    "                               cooldown=0, patience=10, min_lr=0.5e-6)\n",
    "# early_stopper = EarlyStopping(monitor='val_acc', min_delta=1e-4, patience=20)\n",
    "model_checkpoint = ModelCheckpoint('checkpt_digits', monitor='val_acc', save_best_only=True,\n",
    "                                   save_weights_only=True, mode='auto')\n",
    "\n",
    "callbacks = [lr_reducer, model_checkpoint]\n",
    "\n",
    "history = model.fit(x, y, epochs=100, batch_size=32, validation_split=0.2, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 226us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.0213440084457397, 0.621999979019165]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img = cv2.imread(\"/home/kivicode/Downloads/4.png\", cv2.IMREAD_GRAYSCALE)\n",
    "img = 255 - cv2.rotate(cv2.threshold(img, 10, 255, cv2.THRESH_OTSU)[1], rng.choice([0, cv2.ROTATE_180, cv2.ROTATE_90_CLOCKWISE, cv2.ROTATE_90_COUNTERCLOCKWISE]))\n",
    "plt.imshow(img)\n",
    "\n",
    "img = img.reshape(-1, 28, 28, 1) / 255.\n",
    "model.predict(img).argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"digits_0-4_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function GDenseNet in module keras_gcnn.applications.densenetnew:\n",
      "\n",
      "GDenseNet(mc_dropout, padding, nb_dense_block=3, growth_rate=12, nb_filter=-1, nb_layers_per_block=-1, bottleneck=False, reduction=0.0, dropout_rate=0.0, weight_decay=0.0001, subsample_initial_block=False, include_top=True, weights=None, input_tensor=None, pooling=None, classes=10, activation='softmax', input_shape=None, depth=40, bn_momentum=0.99, use_gcnn=False, conv_group=None, depth_multiplier=1, use_g_bn=True, kernel_size=3, mc_bn=None)\n",
      "    Instantiate the DenseNet architecture.\n",
      "    \n",
      "    The model and the weights are compatible with both\n",
      "    TensorFlow and Theano. The dimension ordering\n",
      "    convention used by the model is the one\n",
      "    specified in your Keras config file.\n",
      "    \n",
      "    # Arguments\n",
      "        input_shape: optional shape tuple, only to be specified\n",
      "            if `include_top` is False (otherwise the input shape\n",
      "            has to be `(224, 224, 3)` (with `channels_last` dim ordering)\n",
      "            or `(3, 224, 224)` (with `channels_first` dim ordering).\n",
      "            It should have exactly 3 inputs channels,\n",
      "            and width and height should be no smaller than 8.\n",
      "            E.g. `(224, 224, 3)` would be one valid value.\n",
      "        depth: number or layers in the DenseNet\n",
      "        nb_dense_block: number of dense blocks to add to end\n",
      "        growth_rate: number of filters to add per dense block\n",
      "        nb_filter: initial number of filters. -1 indicates initial\n",
      "            number of filters will default to 2 * growth_rate\n",
      "        nb_layers_per_block: number of layers in each dense block.\n",
      "            Can be a -1, positive integer or a list.\n",
      "            If -1, calculates nb_layer_per_block from the network depth.\n",
      "            If positive integer, a set number of layers per dense block.\n",
      "            If list, nb_layer is used as provided. Note that list size must\n",
      "            be nb_dense_block\n",
      "        bottleneck: flag to add bottleneck blocks in between dense blocks\n",
      "        reduction: reduction factor of transition blocks.\n",
      "            Note : reduction value is inverted to compute compression.\n",
      "        dropout_rate: dropout rate\n",
      "        weight_decay: weight decay rate\n",
      "        subsample_initial_block: Changes model type to suit different datasets.\n",
      "            Should be set to True for ImageNet, and False for CIFAR datasets.\n",
      "            When set to True, the initial convolution will be strided and\n",
      "            adds a MaxPooling2D before the initial dense block.\n",
      "        include_top: whether to include the fully-connected\n",
      "            layer at the top of the network.\n",
      "        weights: one of `None` (random initialization) or\n",
      "            'imagenet' (pre-training on ImageNet)..\n",
      "        input_tensor: optional Keras tensor (i.e. output of `layers.Input()`)\n",
      "            to use as image input for the model.\n",
      "        pooling: Optional pooling mode for feature extraction\n",
      "            when `include_top` is `False`.\n",
      "            - `None` means that the output of the model\n",
      "                will be the 4D tensor output of the\n",
      "                last convolutional layer.\n",
      "            - `avg` means that global average pooling\n",
      "                will be applied to the output of the\n",
      "                last convolutional layer, and thus\n",
      "                the output of the model will be a\n",
      "                2D tensor.\n",
      "            - `max` means that global max pooling will\n",
      "                be applied.\n",
      "        classes: optional number of classes to classify images\n",
      "            into, only to be specified if `include_top` is True, and\n",
      "            if no `weights` argument is specified.\n",
      "        activation: Type of activation at the top layer. Can be one of\n",
      "            'softmax' or 'sigmoid'. Note that if sigmoid is used,\n",
      "             classes must be 1.\n",
      "    \n",
      "    # Returns\n",
      "        A Keras model instance.\n",
      "    \n",
      "    # Raises\n",
      "        ValueError: in case of invalid argument for `weights`,\n",
      "            or invalid input shape.\n",
      "            :param mc_bn:\n",
      "            :param bn_momentum:\n",
      "            :param padding:\n",
      "            :param mc_dropout:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(GDenseNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
